{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103e038f",
   "metadata": {},
   "source": [
    "# AI agent for playing DOOM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57986137",
   "metadata": {},
   "source": [
    "### VIZDOOM allows developing AI bots that play DOOM using visual information (the screen buffer). It is primarily intended for research in machine visual learning, and deep reinforcement learning, in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a2dc4",
   "metadata": {},
   "source": [
    "### Step 1- Getting vizdoom working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d7fd5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vizdoom in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.1.14)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from vizdoom) (1.21.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install vizdoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a53180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import vizddom for gaming env\n",
    "from vizdoom import*\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "014ea309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup game\n",
    "game= DoomGame()\n",
    "game.load_config('C:\\\\Users\\\\Lenovo\\\\Documents\\\\GitHub\\\\ViZDoom\\\\scenarios\\\\basic.cfg')\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7b8ead",
   "metadata": {},
   "source": [
    "This code sets up a VizDoom game environment for the user to interact with. VizDoom is a platform for reinforcement learning research with Doom, a popular first-person shooter video game.\n",
    "\n",
    "The DoomGame() function initializes the game object, while load_config() loads the game configuration file (basic.cfg) from the specified location (github/VizDoom/scenarios/). This configuration file specifies various settings for the game, such as the layout of the game level, the available weapons, and the game's reward system.\n",
    "\n",
    "Finally, init() initializes the game with the specified configuration and prepares it for gameplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b87b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4e769cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions= np.identity(3, dtype=np.uint8) #set of actions we can take in env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a6f2f",
   "metadata": {},
   "source": [
    "This code creates an array of possible actions that the agent can take in the environment.\n",
    "\n",
    "In this case, the array contains 3 possible actions, represented as a 3x3 identity matrix where the diagonal elements are 1 and all other elements are 0. The dtype=np.uint8 argument specifies that the array elements should be of unsigned 8-bit integer type.\n",
    "\n",
    "Each row in the array represents an action that the agent can take. For example, if the agent wants to take the first action, it would select the first row in the array ([1, 0, 0]). Similarly, if it wants to take the second action, it would select the second row in the array ([0, 1, 0]), and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d39c339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "269072c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(actions) # random actions taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d86f8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#game.new_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82df6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#game.is_episode_finished()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91b1a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#game.make_action(random.choice(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88e1c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca70bbc",
   "metadata": {},
   "source": [
    "This code retrieves the current state of the game environment.\n",
    "\n",
    "The get_state() method is called on the game object, which is an instance of the DoomGame class that we initialized earlier. This method returns a GameState object that represents the current state of the game, including information such as the player's position, health, ammunition, and the state of the environment.\n",
    "\n",
    "We can use the GameState object to extract information about the game state that we can use to make decisions about what actions to take. For example, we can extract the player's position to determine where the player is in the game world, or extract the state of the environment to determine the location of obstacles and enemies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7559b34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[35, 39, 39, ..., 39, 39, 39],\n",
       "        [59, 67, 59, ..., 67, 67, 91],\n",
       "        [79, 79, 79, ..., 79, 91, 91],\n",
       "        ...,\n",
       "        [19, 19, 11, ..., 47, 47, 55],\n",
       "        [19, 27, 19, ..., 47, 47, 47],\n",
       "        [11, 19, 19, ..., 27, 19, 19]],\n",
       "\n",
       "       [[35, 39, 39, ..., 39, 39, 39],\n",
       "        [59, 67, 59, ..., 67, 67, 91],\n",
       "        [79, 79, 79, ..., 79, 91, 91],\n",
       "        ...,\n",
       "        [19, 19, 11, ..., 47, 47, 55],\n",
       "        [19, 27, 19, ..., 47, 47, 47],\n",
       "        [11, 19, 19, ..., 27, 19, 19]],\n",
       "\n",
       "       [[35, 39, 39, ..., 39, 39, 39],\n",
       "        [59, 67, 59, ..., 67, 67, 91],\n",
       "        [79, 79, 79, ..., 79, 91, 91],\n",
       "        ...,\n",
       "        [19, 19, 11, ..., 47, 47, 55],\n",
       "        [19, 27, 19, ..., 47, 47, 47],\n",
       "        [11, 19, 19, ..., 27, 19, 19]]], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.screen_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd0edd1",
   "metadata": {},
   "source": [
    "This code retrieves the current screen buffer of the game state.\n",
    "\n",
    "In VizDoom, the screen buffer is a numpy array that represents the current visual state of the game. It contains a sequence of pixels that represent the image of the game world as seen from the player's perspective.\n",
    "\n",
    "The screen_buffer attribute is a property of the GameState object that we obtained earlier using the get_state() method. It is a numpy array of shape (height, width, channels) where height and width represent the dimensions of the game window, and channels represents the number of color channels in the image (typically 3 for RGB).\n",
    "\n",
    "We can use the screen buffer to obtain visual information about the game world, such as the position of enemies or the location of power-ups. We can also use it to display the current state of the game to the user or to an AI agent that is learning to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45a3d6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.game_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df52632",
   "metadata": {},
   "source": [
    "This code retrieves the values of the game variables in the current game state.\n",
    "\n",
    "In VizDoom, game variables are used to represent dynamic aspects of the game state that can change over time, such as the player's health, ammunition, or score.\n",
    "\n",
    "The game_variables attribute is a property of the GameState object that we obtained earlier using the get_state() method. It is a numpy array containing the current values of all the game variables defined in the game configuration file.\n",
    "\n",
    "We can use the values of the game variables to determine the current state of the game and make decisions about what actions to take next. For example, if the player's health is low, the agent may decide to prioritize finding health packs or avoiding enemies to stay alive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79ad5cf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: 99.0\n",
      "Result: -229.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "Result: -375.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: 97.0\n",
      "Result: 72.0\n",
      "reward: -4.0\n",
      "reward: 99.0\n",
      "Result: 95.0\n",
      "reward: -4.0\n",
      "reward: 99.0\n",
      "Result: 95.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: 99.0\n",
      "Result: 62.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "Result: -380.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "Result: -360.0\n",
      "reward: -4.0\n",
      "reward: 99.0\n",
      "Result: 95.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: 99.0\n",
      "Result: -221.0\n"
     ]
    }
   ],
   "source": [
    "episodes= 10\n",
    "for episode in range(episodes):\n",
    "    game.new_episode()\n",
    "    #check the game isn't done\n",
    "    while not game.is_episode_finished():\n",
    "        state= game.get_state()#getting the state\n",
    "        img= state.screen_buffer#get image of the state\n",
    "        info= state.game_variables #get the game variables- ammo\n",
    "        reward= game.make_action(random.choice(actions),4)#taking random action, 4 is for frameskip means we skip 4 frames and then get to the result\n",
    "        print('reward:',reward)\n",
    "        time.sleep(0.02)\n",
    "    print('Result:', game.get_total_reward())\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f0787",
   "metadata": {},
   "source": [
    "this code runs a simple random agent that takes random actions in the game environment and prints out the rewards received for each action and the total reward received for each episode. The purpose of this code is to test the game environment and ensure that it is running correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "630f5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1d20d2",
   "metadata": {},
   "source": [
    "This code closes the VizDoom game instance, freeing up any resources used by the game.\n",
    "\n",
    "When we are finished using the game environment, it is good practice to call game.close() to ensure that all resources associated with the game are properly released. This is especially important when running large-scale experiments or training deep learning models, as leaving game instances open can lead to memory leaks or other performance issues.\n",
    "\n",
    "Closing the game instance will also terminate the Doom engine process and close any windows or graphics associated with the game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56a239",
   "metadata": {},
   "source": [
    "## Step 2- Converting into GYM environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04171b51",
   "metadata": {},
   "source": [
    "Converting the VizDoom game environment into a gym environment allows us to use the same set of APIs and interfaces that are provided by the OpenAI Gym library for other game environments. This makes it easier to write and test reinforcement learning algorithms that can be used with multiple game environments, as we can use the same code for training and evaluating our agents across different environments.\n",
    "Converting the VizDoom environment to a Gym environment involves defining the observation space, action space, and reward function of the environment in terms of the Gym API. This makes it easy to use the environment with standard Gym tools and interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49f9eb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gym) (1.21.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b5aebbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from opencv-python) (1.21.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6be4bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import env base class from open ai gym\n",
    "from gym import Env\n",
    "\n",
    "#import gym spaces\n",
    "from gym.spaces import Discrete,Box #Spaces are crucially used in Gym to define the format of valid actions and observations\n",
    "#They clearly define how to interact with environments, i.e. they specify what actions need to look like and what observations will look like\n",
    "#They allow us to work with highly structured data (e.g. in the form of elements of Dict spaces) and painlessly transform them into flat arrays that can be used in learning code\n",
    "#They provide a method to sample random elements. This is especially useful for exploration and debugging.\n",
    "\n",
    "import cv2 #import opencv\n",
    "#By using it, one can process images and videos to identify objects, faces, or even the handwriting of a human. When it integrated with various libraries, such as Numpy, python is capable of processing the OpenCV array structure for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27ac96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic case\n",
    "\"\"\"The purpose of the scenario is just to check if using this framework to train some AI in a 3D environment is feasible.\n",
    "\n",
    "Map is a rectangle with gray walls, ceiling and floor. Player is spawned along the longer wall, in the center. A red, circular monster is spawned randomly somewhere along the opposite wall. Player can only (config) go left/right and shoot. 1 hit is enough to kill the monster. Episode finishes when monster is killed or on timeout.\n",
    "\n",
    "REWARDS:\n",
    "\n",
    "+101 for killing the monster -5 for missing Episode ends after killing the monster or on timeout.\n",
    "\n",
    "Further configuration:\n",
    "\n",
    "living reward = -1,\n",
    "3 available buttons: move left, move right, shoot (attack)\n",
    "timeout = 300\"\"\"\n",
    "#create vizdoom openai gym environment\n",
    "class VizDoomGym(Env):\n",
    "    def __init__(self, render=False):#function that is called when we start env\n",
    "        \n",
    "        self.game= DoomGame()\n",
    "        self.game.load_config('C:\\\\Users\\\\Lenovo\\\\Documents\\\\GitHub\\\\ViZDoom\\\\scenarios\\\\basic.cfg')\n",
    "        \n",
    "        #render form logic\n",
    "        if render== False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        \n",
    "        self.game.init()\n",
    "        \n",
    "        #create an action space and observation space\n",
    "        self.observation_space= Box(low=0,high=255,shape=(100,160,1),  dtype=np.uint8)\n",
    "        self.action_space= Discrete(3)\n",
    "        \n",
    "    def step(self,action):#how we take a step in env\n",
    "        # Specif action and take step\n",
    "        actions= np.identity(3)\n",
    "        reward= self.game.make_action(actions[action],4)\n",
    "        \n",
    "        #get all the other stuff we need to return\n",
    "        if self.game.get_state():\n",
    "            state= self.game.get_state().screen_buffer\n",
    "            state= self.grayscale(state)\n",
    "            ammo= self.game.get_state().game_variables[0]\n",
    "            info = ammo\n",
    "        else:\n",
    "            state= np.zeros(self.observation_space.shape)\n",
    "            info=0\n",
    "         \n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state,reward,done,info\n",
    "        \n",
    "    def render():\n",
    "        pass\n",
    "    def reset(self): #hppens when start a new game\n",
    "        self.game.new_episode()\n",
    "        state= self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state)\n",
    "        \n",
    "        \n",
    "    def grayscale(self,observation):\n",
    "        gray= cv2.cvtColor(np.moveaxis(observation,0,-1),cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160,1))\n",
    "        return state\n",
    "        \n",
    "    def close(self): #close the game\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a7f32",
   "metadata": {},
   "source": [
    "The code defines a custom OpenAI Gym environment for the VizDoom game. The environment is defined in a class called VizDoomGym, which inherits from the OpenAI Gym Env class.\n",
    "\n",
    "__init__(self, render=False)-  is the constructor function that initializes the DoomGame object and sets up the action and observation spaces. Here's what each part does:\n",
    "\n",
    "self.game = DoomGame() creates a DoomGame object.\n",
    "self.game.load_config('C:\\\\Users\\\\Lenovo\\\\Documents\\\\GitHub\\\\ViZDoom\\\\scenarios\\\\basic.cfg') loads the configuration file for the game.\n",
    "The render parameter is used to decide whether to show the game window or not.\n",
    "If render is False, the game window is not shown.\n",
    "self.observation_space is the observation space of the environment. It is set to a Box object with a low value of 0 and a high value of 255. \n",
    "The shape of the observation space is (100, 160, 1), which means that it is a grayscale image with a height of 100 pixels and a width of 160 pixels.\n",
    "\n",
    "step(self, action)- is the function that takes a step in the environment. Here's what each part does:\n",
    "\n",
    "actions = np.identity(3) creates an identity matrix of shape (3, 3). This is used to convert the action index into an actual action vector.\n",
    "reward = self.game.make_action(actions[action], 4) takes a step in the game by executing the action corresponding to the action parameter.\n",
    "if self.game.get_state(): checks if the game state exists. If it does, it retrieves the screen buffer and converts it to grayscale using the grayscale() function.\n",
    "ammo = self.game.get_state().game_variables[0] retrieves the number of bullets the player has left from the game state.\n",
    "info = ammo, sets the info variable to the number of bullets the player has left.\n",
    "info = {\"info\": info} creates a dictionary containing the info variable.\n",
    "done = self.game.is_episode_finished() checks if the episode is finished.\n",
    "The function returns state, reward, done, and info.\n",
    "\n",
    "grayscale(self, observation)- is a function that converts a color image to a grayscale image. Here's what each part does:\n",
    "\n",
    "gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY) converts the color image to grayscale using OpenCV.\n",
    "resize = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_CUBIC) resizes the grayscale image to the desired size of (160, 100).\n",
    "state = np.reshape(resize, (100, 160, 1)) reshapes the grayscale image to the desired shape of (100, 160, 1).\n",
    "The function returns the reshaped grayscale image.\n",
    "\n",
    "The reset() method is called to reset the environment and start a new episode. It initializes a new episode in the VizDoom game and returns the initial state of the game as the initial observation.\n",
    "\n",
    "Finally, the close() method is called to close the VizDoom game when the environment is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfc9c964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class VizDoomGym(Env): \\n    # Function that is called when we start the env\\n    def __init__(self, render=False, config=\\'github/VizDoom/scenarios/deadly_corridor_s1.cfg\\'): \\n        # Inherit from Env\\n        super().__init__()\\n        # Setup the game \\n        self.game = DoomGame()\\n        self.game.load_config(config)\\n        \\n        # Render frame logic\\n        if render == False: \\n            self.game.set_window_visible(False)\\n        else:\\n            self.game.set_window_visible(True)\\n        \\n        # Start the game \\n        self.game.init()\\n        \\n        # Create the action space and observation space\\n        self.observation_space = Box(low=0, high=255, shape=(100,160,1), dtype=np.uint8) \\n        self.action_space = Discrete(7)\\n        \\n        # Game variables: HEALTH DAMAGE_TAKEN HITCOUNT SELECTED_WEAPON_AMMO\\n        self.damage_taken = 0\\n        self.hitcount = 0\\n        self.ammo = 52 ## CHANGED\\n        \\n        \\n    # This is how we take a step in the environment\\n    def step(self, action):\\n        # Specify action and take step \\n        actions = np.identity(7)\\n        movement_reward = self.game.make_action(actions[action], 4) \\n        \\n        reward = 0 \\n        # Get all the other stuff we need to retun \\n        if self.game.get_state(): \\n            state = self.game.get_state().screen_buffer\\n            state = self.grayscale(state)\\n            \\n            # Reward shaping\\n            game_variables = self.game.get_state().game_variables\\n            health, damage_taken, hitcount, ammo = game_variables\\n            \\n            # Calculate reward deltas\\n            damage_taken_delta = -damage_taken + self.damage_taken\\n            self.damage_taken = damage_taken\\n            hitcount_delta = hitcount - self.hitcount\\n            self.hitcount = hitcount\\n            ammo_delta = ammo - self.ammo\\n            self.ammo = ammo\\n            \\n            reward = movement_reward + damage_taken_delta*10 + hitcount_delta*200  + ammo_delta*5 \\n            info = ammo\\n        else: \\n            state = np.zeros(self.observation_space.shape)\\n            info = 0 \\n        \\n        info = {\"info\":info}\\n        done = self.game.is_episode_finished()\\n        \\n        return state, reward, done, info \\n    \\n    # Define how to render the game or environment   \\n    def render(): \\n        pass\\n    \\n    # What happens when we start a new game \\n    def reset(self): \\n        self.game.new_episode()\\n        state = self.game.get_state().screen_buffer\\n        return self.grayscale(state)\\n    \\n    # Grayscale the game frame and resize it \\n    def grayscale(self, observation):\\n        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\\n        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\\n        state = np.reshape(resize, (100,160,1))\\n        return state\\n    \\n    # Call to close down the game\\n    def close(self): \\n        self.game.close()'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in the case of DEADLY CORRIDOR\n",
    "\n",
    "#he purpose of this scenario is to teach the agent to navigate towards his fundamental goal (the vest) and make sure he survives at the same time.\n",
    "\n",
    "#Map is a corridor with shooting monsters on both sides (6 monsters in total). A green vest is placed at the opposite end of the corridor. Reward is proportional (negative or positive) to change of the distance between the player and the vest. If player ignores monsters on the sides and runs straight for the vest he will be killed somewhere along the way. To ensure this behavior doom_skill = 5 (config) is needed.\n",
    "\n",
    "#REWARDS:\n",
    "\n",
    "#+dX for getting closer to the vest. -dX for getting further from the vest.\n",
    "\n",
    "#Further configuration:\n",
    "\n",
    "#5 available buttons: turn left, turn right, move left, move right, shoot (attack)\n",
    "#timeout = 4200\n",
    "#death penalty = 100\n",
    "#doom_skill = 5\n",
    "\n",
    "\n",
    "# Create Vizdoom OpenAI Gym Environment\n",
    "\n",
    "\"\"\"class VizDoomGym(Env): \n",
    "    # Function that is called when we start the env\n",
    "    def __init__(self, render=False, config='github/VizDoom/scenarios/deadly_corridor_s1.cfg'): \n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        # Setup the game \n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(config)\n",
    "        \n",
    "        # Render frame logic\n",
    "        if render == False: \n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        \n",
    "        # Start the game \n",
    "        self.game.init()\n",
    "        \n",
    "        # Create the action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100,160,1), dtype=np.uint8) \n",
    "        self.action_space = Discrete(7)\n",
    "        \n",
    "        # Game variables: HEALTH DAMAGE_TAKEN HITCOUNT SELECTED_WEAPON_AMMO\n",
    "        self.damage_taken = 0\n",
    "        self.hitcount = 0\n",
    "        self.ammo = 52 ## CHANGED\n",
    "        \n",
    "        \n",
    "    # This is how we take a step in the environment\n",
    "    def step(self, action):\n",
    "        # Specify action and take step \n",
    "        actions = np.identity(7)\n",
    "        movement_reward = self.game.make_action(actions[action], 4) \n",
    "        \n",
    "        reward = 0 \n",
    "        # Get all the other stuff we need to retun \n",
    "        if self.game.get_state(): \n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)\n",
    "            \n",
    "            # Reward shaping\n",
    "            game_variables = self.game.get_state().game_variables\n",
    "            health, damage_taken, hitcount, ammo = game_variables\n",
    "            \n",
    "            # Calculate reward deltas\n",
    "            damage_taken_delta = -damage_taken + self.damage_taken\n",
    "            self.damage_taken = damage_taken\n",
    "            hitcount_delta = hitcount - self.hitcount\n",
    "            self.hitcount = hitcount\n",
    "            ammo_delta = ammo - self.ammo\n",
    "            self.ammo = ammo\n",
    "            \n",
    "            reward = movement_reward + damage_taken_delta*10 + hitcount_delta*200  + ammo_delta*5 \n",
    "            info = ammo\n",
    "        else: \n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0 \n",
    "        \n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state, reward, done, info \n",
    "    \n",
    "    # Define how to render the game or environment   \n",
    "    def render(): \n",
    "        pass\n",
    "    \n",
    "    # What happens when we start a new game \n",
    "    def reset(self): \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state)\n",
    "    \n",
    "    # Grayscale the game frame and resize it \n",
    "    def grayscale(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160,1))\n",
    "        return state\n",
    "    \n",
    "    # Call to close down the game\n",
    "    def close(self): \n",
    "        self.game.close()\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8470e980",
   "metadata": {},
   "source": [
    "This code defines a custom OpenAI Gym environment called VizDoomGym, which is based on the VizDoom game. The environment is designed to teach an agent to navigate through a corridor with shooting monsters on both sides, and to reach a green vest placed at the opposite end of the corridor while avoiding getting killed. The environment has 5 available buttons that the agent can use to turn left, turn right, move left, move right, or shoot (attack). The maximum duration of a game episode is set to 4200 steps, and the penalty for dying is 100. The doom_skill parameter is set to 5, which affects the difficulty level of the game.\n",
    "\n",
    "This function is called when the environment is created. It initializes the DoomGame object and loads the scenario specified in the config parameter. It also sets up the game to either render or not depending on the value of render. It then creates the observation and action spaces, as well as initializes the values of damage_taken, hitcount, and ammo.\n",
    "\n",
    "step(self, action): This function is called when an action is taken in the environment. It takes an action index as input and applies the corresponding action to the game. It then calculates the reward based on the change in game variables such as damage_taken, hitcount, and ammo, as well as the movement reward obtained from the game. Finally, it returns the new state, the reward, whether the episode is finished or not, and any additional information.\n",
    "\n",
    "grayscale(self, observation): This function takes in a game frame observation and converts it to grayscale, resizes it to (160,100), and reshapes it to (100,160,1). This is done to reduce the size of the observation and make it easier to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "766699a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Create Vizdoom OpenAI Gym Environment\\nclass VizDoomGym(Env): \\n    # Function that is called when we start the env\\n    def __init__(self, render=False): \\n        # Inherit from Env\\n        super().__init__()\\n        # Setup the game \\n        self.game = DoomGame()\\n        self.game.load_config(\\'github/VizDoom/scenarios/defend_the_center.cfg\\')\\n        \\n        # Render frame logic\\n        if render == False: \\n            self.game.set_window_visible(False)\\n        else:\\n            self.game.set_window_visible(True)\\n        \\n        # Start the game \\n        self.game.init()\\n        \\n        # Create the action space and observation space\\n        self.observation_space = Box(low=0, high=255, shape=(100,160,1), dtype=np.uint8) \\n        self.action_space = Discrete(3)\\n        \\n    # This is how we take a step in the environment\\n    def step(self, action):\\n        # Specify action and take step \\n        actions = np.identity(3)\\n        reward = self.game.make_action(actions[action], 4) \\n        \\n        # Get all the other stuff we need to retun \\n        if self.game.get_state(): \\n            state = self.game.get_state().screen_buffer\\n            state = self.grayscale(state)\\n            ammo = self.game.get_state().game_variables[0]\\n            info = ammo\\n        else: \\n            state = np.zeros(self.observation_space.shape)\\n            info = 0 \\n        \\n        info = {\"info\":info}\\n        done = self.game.is_episode_finished()\\n        \\n        return state, reward, done, info \\n    \\n    # Define how to render the game or environment \\n    def render(): \\n        pass\\n    \\n    # What happens when we start a new game \\n    def reset(self): \\n        self.game.new_episode()\\n        state = self.game.get_state().screen_buffer\\n        return self.grayscale(state)\\n    \\n    # Grayscale the game frame and resize it \\n    def grayscale(self, observation):\\n        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\\n        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\\n        state = np.reshape(resize, (100,160,1))\\n        return state\\n    \\n    # Call to close down the game\\n    def close(self): \\n        self.game.close()'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In the case of defend center\n",
    "\"\"\"The purpose of this scenario is to teach the agent that killing the monsters is GOOD and when monsters kill you is BAD. In addition, wasting ammunition is not very good either. Agent is rewarded only for killing monsters so he has to figure out the rest for himself.\n",
    "\n",
    "Map is a large circle. Player is spawned in the exact center. 5 melee-only, monsters are spawned along the wall. Monsters are killed after a single shot. After dying each monster is respawned after some time. Episode ends when the player dies (it's inevitable because of limited ammo).\n",
    "\n",
    "REWARDS: +1 for killing a monster\n",
    "\n",
    "Further configuration:\n",
    "\n",
    "3 available buttons: turn left, turn right, shoot (attack)\n",
    "death penalty = 1\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"# Create Vizdoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    # Function that is called when we start the env\n",
    "    def __init__(self, render=False): \n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        # Setup the game \n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('github/VizDoom/scenarios/defend_the_center.cfg')\n",
    "        \n",
    "        # Render frame logic\n",
    "        if render == False: \n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        \n",
    "        # Start the game \n",
    "        self.game.init()\n",
    "        \n",
    "        # Create the action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100,160,1), dtype=np.uint8) \n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "    # This is how we take a step in the environment\n",
    "    def step(self, action):\n",
    "        # Specify action and take step \n",
    "        actions = np.identity(3)\n",
    "        reward = self.game.make_action(actions[action], 4) \n",
    "        \n",
    "        # Get all the other stuff we need to retun \n",
    "        if self.game.get_state(): \n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)\n",
    "            ammo = self.game.get_state().game_variables[0]\n",
    "            info = ammo\n",
    "        else: \n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0 \n",
    "        \n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state, reward, done, info \n",
    "    \n",
    "    # Define how to render the game or environment \n",
    "    def render(): \n",
    "        pass\n",
    "    \n",
    "    # What happens when we start a new game \n",
    "    def reset(self): \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state)\n",
    "    \n",
    "    # Grayscale the game frame and resize it \n",
    "    def grayscale(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160,1))\n",
    "        return state\n",
    "    \n",
    "    # Call to close down the game\n",
    "    def close(self): \n",
    "        self.game.close()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2436dd9",
   "metadata": {},
   "source": [
    "This code defines a VizDoom Gym environment for the \"defend the center\" scenario. The purpose of this scenario is to teach the agent that killing the monsters is good, getting killed is bad, and wasting ammunition is also not good. The agent is rewarded only for killing monsters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd6a011",
   "metadata": {},
   "source": [
    "overview of the differences between the three codes for basic, defend corridor and defend the center scenarios:\n",
    "\n",
    "Basic Scenario: In this scenario, the player is in a simple room with a single monster that does not respawn. The player has to kill the monster to win the game. The player is rewarded for killing the monster and penalized for dying. The player has access to three actions: moving forward, turning left, and turning right.\n",
    "\n",
    "Defend Corridor Scenario: In this scenario, the player is in a corridor and has to defend themselves from a series of monsters that spawn at the end of the corridor. The player has a limited amount of ammunition and is rewarded for killing monsters. The game ends when the player runs out of ammo or dies. The player has access to four actions: moving forward, turning left, turning right, and shooting.\n",
    "\n",
    "Defend the Center Scenario: In this scenario, the player is in the center of a circular arena and is surrounded by monsters that respawn after they are killed. The player has a limited amount of ammunition and is rewarded for killing monsters. The game ends when the player dies. The player has access to three actions: turning left, turning right, and shooting.\n",
    "\n",
    "The code for each scenario reflects these differences. For example, the VizDoomGym class for the Basic Scenario only has three available actions, while the VizDoomGym class for the Defend Corridor Scenario has four available actions, including shooting. Similarly, the reset() and step() methods in the VizDoomGym class are different for each scenario because of the different game mechanics and objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9837a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32646fbd",
   "metadata": {},
   "source": [
    "This code creates an instance of the VizDoomGym environment with the render argument set to True. This means that when the environment is run, the game screen will be displayed for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7bba190",
   "metadata": {},
   "outputs": [],
   "source": [
    "state= env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a526cc2",
   "metadata": {},
   "source": [
    "env.reset() resets the environment and returns the initial state of the environment. In this case, it returns the initial grayscale state of the VizDoom game.\n",
    "\n",
    "The state variable will contain a numpy array of shape (100,160,1) representing the grayscale screen buffer of the game. The screen buffer is a snapshot of the current state of the game that the agent can use as input to make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b34fa1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[55],\n",
       "        [50],\n",
       "        [59],\n",
       "        ...,\n",
       "        [57],\n",
       "        [57],\n",
       "        [66]],\n",
       "\n",
       "       [[68],\n",
       "        [65],\n",
       "        [65],\n",
       "        ...,\n",
       "        [56],\n",
       "        [67],\n",
       "        [72]],\n",
       "\n",
       "       [[49],\n",
       "        [79],\n",
       "        [66],\n",
       "        ...,\n",
       "        [79],\n",
       "        [51],\n",
       "        [29]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[75],\n",
       "        [63],\n",
       "        [62],\n",
       "        ...,\n",
       "        [44],\n",
       "        [71],\n",
       "        [60]],\n",
       "\n",
       "       [[15],\n",
       "        [48],\n",
       "        [47],\n",
       "        ...,\n",
       "        [49],\n",
       "        [69],\n",
       "        [47]],\n",
       "\n",
       "       [[22],\n",
       "        [14],\n",
       "        [26],\n",
       "        ...,\n",
       "        [57],\n",
       "        [37],\n",
       "        [39]]], dtype=uint8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bba13458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 160, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff593c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 1, 100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.moveaxis(state,0,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce04fbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[55],\n",
       "         [50],\n",
       "         [59],\n",
       "         ...,\n",
       "         [57],\n",
       "         [57],\n",
       "         [66]],\n",
       " \n",
       "        [[68],\n",
       "         [65],\n",
       "         [65],\n",
       "         ...,\n",
       "         [56],\n",
       "         [67],\n",
       "         [72]],\n",
       " \n",
       "        [[49],\n",
       "         [79],\n",
       "         [66],\n",
       "         ...,\n",
       "         [79],\n",
       "         [51],\n",
       "         [29]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[75],\n",
       "         [63],\n",
       "         [62],\n",
       "         ...,\n",
       "         [44],\n",
       "         [71],\n",
       "         [60]],\n",
       " \n",
       "        [[15],\n",
       "         [48],\n",
       "         [47],\n",
       "         ...,\n",
       "         [49],\n",
       "         [69],\n",
       "         [47]],\n",
       " \n",
       "        [[22],\n",
       "         [14],\n",
       "         [26],\n",
       "         ...,\n",
       "         [57],\n",
       "         [37],\n",
       "         [39]]], dtype=uint8),\n",
       " -4.0,\n",
       " False,\n",
       " {'info': 50.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637cfd2",
   "metadata": {},
   "source": [
    "This code is calling the step() function of the VizDoomGym environment object env, with an action value of 2.\n",
    "\n",
    "running env.step(2) will make the player shoot their weapon and update the environment state accordingly, returning information about the new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9e910ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[55],\n",
       "        [50],\n",
       "        [59],\n",
       "        ...,\n",
       "        [57],\n",
       "        [57],\n",
       "        [66]],\n",
       "\n",
       "       [[68],\n",
       "        [65],\n",
       "        [65],\n",
       "        ...,\n",
       "        [56],\n",
       "        [67],\n",
       "        [72]],\n",
       "\n",
       "       [[49],\n",
       "        [79],\n",
       "        [66],\n",
       "        ...,\n",
       "        [79],\n",
       "        [51],\n",
       "        [29]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[75],\n",
       "        [63],\n",
       "        [62],\n",
       "        ...,\n",
       "        [44],\n",
       "        [71],\n",
       "        [60]],\n",
       "\n",
       "       [[15],\n",
       "        [48],\n",
       "        [47],\n",
       "        ...,\n",
       "        [49],\n",
       "        [69],\n",
       "        [47]],\n",
       "\n",
       "       [[22],\n",
       "        [14],\n",
       "        [26],\n",
       "        ...,\n",
       "        [57],\n",
       "        [37],\n",
       "        [39]]], dtype=uint8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76d22dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Environment checker\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468aec45",
   "metadata": {},
   "source": [
    "it checks that the environment has an observation space and action space that are compatible with Stable Baselines and that the environment is compatible with multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fc61a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e5684",
   "metadata": {},
   "source": [
    "env_checker.check_env(env) is checking whether the VizDoomGym environment is compatible with Stable Baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15776a8e",
   "metadata": {},
   "source": [
    "## Step 3- View game state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d79bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e822a623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1fbe8d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48b0690d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16ec2655e20>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAFlCAYAAABLDIrrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAACQfElEQVR4nO2de7SV9XnnH845gKCGJBo5HkEFxYjgBcGaqhOdJDLR2DQrHZNoYpzJmlk6mguxK16StCWZEaJddWxDNTXTZrVNrXbNmMSxTSo1iiJLRQQveMELIl4I3sIx3vAc9vyRtd/5vA/7+/puwO32+P2u5Vo/9nnf3+X5Xfb2+/09zzOq0Wg0wjAMwzAMo0Poebs7YBiGYRjGuwv+8WEYhmEYRkfhHx+GYRiGYXQU/vFhGIZhGEZH4R8fhmEYhmF0FP7xYRiGYRhGR+EfH4ZhGIZhdBT+8WEYhmEYRkfhHx+GYRiGYXQU/vFhGIZhGEZH8Zb9+LjssstiypQpsdNOO8Xs2bPjlltueauaMgzDMAzjHYS+t6LSq6++OubNmxeXXXZZHH300fFXf/VXccIJJ8T9998fe++9d+W7W7Zsiaeffjp23XXXGDVq1FvRPcMwDMMwdjAajUa89NJLMTAwED091dzGqLcisdyRRx4Zhx9+eFx++eXFZ9OnT49PfepTsXDhwsp3n3zyyZg8efKO7pJhGIZhGB3A+vXrY9KkSZXP7HDmY/PmzbFixYo4//zzS5/PnTs3li1bttXzr7/+erz++uvFv5u/hQ477LDo7e2NoaGh4m877bRTUR4eHi7KW7ZsKcqjR48u1c+/jR07tigPDAwU5XXr1hXlN954oygr5qW3t3er/uY+RUT09fW1fO79739/Ud51112L8pNPPlmUaZMxY8YUZdqj6pcl36fdiPe9731FeZdddinKTz31VMt6VHv5c9qN/aXd+Mxuu+1WlMePH1+Un3766aL8m9/8pijTHrRxRHkO2C9lN/aJ9uB8bdq0qShv3ry55RhyX7juWGa9bJvYZ599ijLH/etf/7plPyLK4+Y6Z3t1+sEx8X8CXn755aL83HPPldpW//9CO6s1oZ5hmfZ45plnSm28+uqrLcdBe7CuOmuC5Z133rkoc51GRGzYsKFlPxTUWF977bWizLWt+pH7wn5wvaj1VWde1JqIiHjPe95TlLnmX3nllaLM9cL1kfdrqzZ45lSd53vttVdR5rn90ksvFWXalu+q8anvldz37ekHn+FeJTgXEXp9sr/8suf58OKLL7Z8nuWq+Vbrghg3blzx7LJly0rfawo7/MfHc889F8PDwzFx4sTS5xMnTixtkiYWLlwY3/nOd7b6vLe3N/r6+uTCVQsmL24uEv6Ni1otKk6y6kcVcaSeY9uqH5xwVY9aCFXvE+32o85hFlGemzrzV6cfbJuf57GpLxplN5bZtuq3+lLLz3Ed8Z06B3Ede+Qfuqof6sfxjpqXiK0P6SbUgVbnh4/qR25brRFCHdyqH3xGnRn5b8rmaj3yc7W2VT9yX5QNlD3qzItaE/lvqh9qL9T58VF1dlX9j+ab9UN9T6h1V/U/k9vTjzr2yN8ran0Sdc4v9T/UVc/UObOqzmGFt+TOR6vGG41Gyw5dcMEFcc455xT/HhwcjMmTJ8euu+4afX19lYfs9vTp0UcfLcqc2A984ANFmb/A1QG7LRgcHCzK/L/I9773vUWZ/3fB/6vi/xlVQbEdtMHzzz9flPnreI899ijK/L/+bVHo1C979mPjxo1FufkLOkL/H9a2gPZQX8jPPvtsUSYLQpurg2Z7wTkmA7b77rsX5aovQgX1RUjwc/7f669+9auizHlheVug5oJfOuwT12YeA5ky1Qb3Lt9Xa5Nzwf9j5V7Idal+EGou2A/+X3HV/1S98MILLevinqnTNsF+qLmIKH/58f/u69iD/VA/BLnfMqPEPpJd4VnNfqi1quzBchW7yH3Jc1TV1e7+yWuTduBa4PcVv0vq/E9EXbAv7AfZuGa9b3bPg9jhPz5233336O3t3Yrl2Lhx41ZsSMRvB6YOAcMwDMMwRh52uKvtmDFjYvbs2bF48eLS54sXL46jjjpqRzdnGIZhGMY7DG+J7HLOOefEaaedFnPmzInf/d3fjSuuuCKeeOKJOPPMM2vX8cILL0Rvb2/JNZcU3+OPP16U60ozdSg/XtgiU0Mai5Qnqa4qnY5Q90pI+/NCLC+XcdykyFmualvZgM+TWttvv/2KMsfNS7r5cpS6yKSkK9qWF5Vo/wkTJhTl9evXF2XaLKLe3RzVP1KS6kIsJYDM7pGmVfq+mnv2g5Q15R8yhJRmIsqSANtTNlBSBO1Pe5AKz5c+1YW2dm3Acn9/f8vPKYNElCXM7bEBx0o7c/3n9Uu5kGeT2mPqsiv7yovfLHP9R5RlY9L+6sK8Ou9UXykBT5s2rdQ2559nAudC3WlRdyr4DNvOFxf5b9q/1X3C3B7Hp9Y/JZEsHU2dOrUo8xzm3OdzuAllA64Drrt8wZjfgzxnHnnkkZb9qHPvSD3DdRdRlph4JvMMas5FO9cT3pIfH5/97Gfj+eefj+9+97vxzDPPxMyZM+Nf/uVfSrfWDcMwDMN4d+Itu3B61llnxVlnnfVWVW8YhmEYxjsUb9mPj+3Fs88+Gz09PSUqlczJ9OnTizLjH/CGfkSZBqtyo2qCMgppxD333LMoUxLhrXpKNrkudcOc/SBlyhgX7AdjL3BsWX5QN7DrtE06bs2aNS3bJhWbJQC23a6XEGlcUuG0M71x8o10zkH+Wyuwf3yetCepflLCnJequhRofz5PjwXag9R7pmU5blK5dfqhPE4I1b/cRxXfRIE24HwrOSzf1lfxJFiuYwPS0aSZuS8ypc6+Kxmljg04JlLe7Adj7+T2uD7r7HWCkhQpdcYioqwWUbY5z17ltqtsQCmP65n9yOuR5yLP+jquncrVnLIqY3Zk6ZpSV7ttqxg7bJvlLH1Qaua5Qy+fOqA9ub6q4tlwf99zzz1FudW+aue8d2I5wzAMwzA6Cv/4MAzDMAyjo+ha2aXRaESj0SjRSg8++GBRJgXN8oEHHliqhxQhbyiroDSKrl27dm1RJtVIejInzSNtR6qsTihmUqmke9kntpfz4SjvBDVW1Tb7yrYpgdEGEeVb26RJ67StvIpIF1L2ojdIfp/yD1Gnbb5LWpzyW25beS7VgRo36VcVwCiifEtfeX2o9lhWa5P9YP+qUMfOpGlVYDyu/ywfKKlFQbXNNazq5FkSoW1VJ6AX26aUx37wjMrSBz0b2m2bIPXO9cyx5bWsApy12zbpfZ5frD+3rQLOqbb5OfcIbc62KW/k+a0KdtcKbJueMzxDaAN+x+TrA0reqrO/eR7vv//+RZmSG2W91atXl+pSsl4rO7cTiNLMh2EYhmEYHYV/fBiGYRiG0VGMamxLwo63EIODgzFhwoQ4+OCDo7e3V96GV5kgMy1LKpG0rsoQqgLAKDNVJXKiHMS+MyAO31FJ3HjjXiUPym2T2mMwHhUIq922FW0cUbYzJQDSyCrAUJ32ON/0fIkoU6X0NmIbKuMv21ZBgZTXTUSZIiZFr8anAjGptUaKPHt9qCBLKj8I7aTSG7ANtp0pf/5brSnWRRuoIFy0M6UIruWI8npW+6FO4DO+S48HehHlcSsvITVWFfiM9ud5Ram2qm0lOag1xTJzg7Af6syI0HuRYyVU4jxmYaXMSZvnvaC8iuokf6R0SDtz73Cs+btE5TihDZSsRw8enj/8HlPnY0R5TbGP6pxSXkyUjpSkns8DzuubJVUdHh6O++67LzZt2iTzDDVh5sMwDMMwjI7CPz4MwzAMw+go/OPDMAzDMIyOomvvfBx33HHR19cnIybWca3Kf6PGRe2KdxaowamIeMq9Kbet3mdyJGptanxEXdc2uifSnYv6Xbu2JVSitlyXct/bUe1lnZlj5VyqyHt11pFaQ3TdiyhHCaT96+jzqk8qMmXWU3fUWFXUTt67qHIVb3esBMdAjZv7Jbe9PceXiubKexDZxXVHgWOlbVVisu2FuuNDF1DeOdjerwV1r4dzyXtRddZH7pd6R9mWa7vdyKBVUPfleD7Q/u266eZ31N0a3jFs17ZVduW/uT45l81xDw0NxQ033OA7H4ZhGIZhdB/848MwDMMwjI6iayOcPv3009Hb21uK4km6iW6NLFdRRgRpJtLl++67b8t36famonZW0Vtsj3QU3c3oCsaopGyPqNse+842+vv737S9unQoodwt6ZrKNjh/Smao01YGJR+6KjKaX52kY6o9zl1EmWYl9cuIjSoypQJpXI6HlGdEmQ6lVNBue1w3dNGrkrrotl4noWGd9rgnafPsaquibdZpjxId9yTXLNvLEky7e4NzRjmAc0z3cLqpt7svcnscK9tmUki6nLabEDKi7B5N6YrnzBNPPFGUue+3pT2CbaiIpVybSrqoAtcOQxnwO4o2pG3rRLVWbUWUozrTtnRLfuihh4qySqhaB1ku4Xci5VC215w/J5YzDMMwDKNr4R8fhmEYhmF0FF0ru7z00kvR09MT9913X/EZKZ8pU6YUZVJgpNQjyrd+VbRBUmIPPPBAy/ZIPZHCJM1GyjRCR1MkVcb2SKdRoiDdThqY7VXJTaTeKcHQHqQO2W/eCq9qjyBlSOqRdbE9Ff1VeSOotiLK8pFKJEUKm/JPHWqb7WU5jJEOSV2SiqQcU2d8fJfyRo4qy5vubINz3y79yv6xH/SQiCiv57oy5Ju1x7kjPU8bb297BPc02+PZUCXxKfAd7mmuD+4r2pI2rzs2PsfzhDIWk5ax7Xa9k/JzPIc5PkotbLvO3OX9zbOQ4+N62VHt5T3G7wBKYmyP51ed9lSk6P3226/0HPf+mjVrirLy2lFrlf2gVMUyvyPqtteEZRfDMAzDMLoW/vFhGIZhGEZH0bVBxmbOnLlVsjR2VdE72QtAeXRQjqmTbEolNOLN6kx1rV27tijnRGCt6lKJzUiZkrolPZYTi6lkRyqgFPvOJGK8Gc/xqAR1VWNSieL22WefokzJgBIa362iM1XSJdqQXiqPPvpoy76qNVElzbA9rguuNdK1dRJE0R4MDkWvpYjynLENros6a0LtK66DvMc4TyrgEqHWHUHamV4K2dtFjYll2pnriDQ37cn2SHfTmyOi3l5S3lFca5QGlP2q1jltoNrjmCi1UK5QAeLymmAb9FDi51x3KokkoeyXZTZ6fXAtKDlTJdfjmNgepaNsc0qYdWyoxsQAl5SnOFauiTymN0vuFlEeH9ugbEVPMY4ny6qEGl9zbwwPD8fKlSsdZMwwDMMwjO6Df3wYhmEYhtFRdK23y7777hujR48u0UGkDhWNRYo7IuKRRx4pyqStp02bVpR5u71OG6Tj6M2RoeQL3o7m7WHVBsdEmYA39GfPnl1qe926dUWZwZFYLyloUmiKoj3ggAOKMmnBquBLpBLZBp+hnEM7cY6U10ym7VV79CJgXbzBTpupW93KZhFlSvLxxx9vOQ7Sp2xDUdNKVuB4cttcF3yO1DTrUhITwb6Suo0or1vlIaPaUAHplATAtnIbtBvtqdYEaWFKmAzQxPbyfCt5hc+Rkmf/KAFz3EqGym1zHVLC4Vojrc7x8d06UkQGZVIlRRBKJlDePDynKV9GlM9Ormd1Vqv8WkrK5txn6YPgvCoplu3Ru4/nOaUj2iDLU2o+aEPKgjzXaDOecayT9qjK1cXnKIFxPdeFmQ/DMAzDMDoK//gwDMMwDKOj6FrZZfny5dHT01OikEklMfhPHbo8gzf036o2SEOyDVKJbEPJMQqkVVmOKN90J4XXbhuK/lb1b0sbBOlh0n2k+CibZGq0TnAkeoOom+5cHypQURVoE75PyYJjVfSrQpYf6K2h8rC0m3dF3crPeTG2pw2uKdLf9OZQeYoi2g9ExzZoJ5bp4VKn/qo2VPAyzne7we0iyl4mPLNYL/OatNsG66fMElGe43bbIJS8Qm8LBrjKbddZX3XaoOzbbv3b20bdwHVsgxIO2+B31NKlS9tqQ9Vf1cZtt922VRsOMmYYhmEYRtfCPz4MwzAMw+goulZ2GTNmTPT09JRuU999991FmVQQKfkcIIX0OelU3oBnGytXrizKpLJ5i5zPM79HppxIW/Md5qshDb///vsXZXop8Cax8hjJdDTboAfJYYcdVpQfe+yxoky6VgVEo/3YXs6BcMghhxRllWdB3YBnmZQu7UdakDfeI8p2U3QjqXRSoLzBzlwpKh9OFS2r2uCNdM49x1fH6yOvNcpunG+Vml4FXyLYBuvP3i512iCUJwptw/XBtZnHrQJx7ag2qgJ9qVw0tL/KI1Tl2dBElZcP5Rz2I+cbaqcNjoFSC8+JCO09p7ye2DY9ALmPKUuwvWzzKm+zJigZsQ3Wq9qoCuDF55hbjHajl5tKa5/nslV7/C6IKK9bejxStqedlScWAwTSNuwrpZUIHfyyam/UgZkPwzAMwzA6Cv/4MAzDMAyjo+ja3C4nnHBCjB49uiQ5kHIjTUTaNw+H1JzKC8Bn1G1dRSmScst0NCUf9b6SOEh10bNE9TvTqgxkQ/vQi4CyEmUGUrcqcBORx8A2ePubsosKaMR5pW1VvpMchIjrRQV+IjgvtDnlO/aDnjKZaiQFqvpO7x/OKz2jlBcGkee7zpqk/WmbOv2mzbP3g1qTfF+1QZsr6YJzWjVugvNKm3O9UA5TuZtUvzNoT3q7cC+pgGqq39xH3MMZfE71neuZZyTnlTQ8Zbbs3VRnL7FP9KSgRxm91tQZkMet+k6PH86rso3KOcXncy4fzitz16h8JwRtw3nhXqKsnb0E2XdlH9qDa3V7+h1R3nPK5s31MjQ0FLfeeqtzuxiGYRiG0X3wjw/DMAzDMDqKrpVd9txzz+jp6SndXP7ABz5QlEl/k5bNFKG6AU9KiHQj6T/eXKY3Qt3AMP39/UWZt7x5szvnomkFUrSkjWmPHGSsygunCU497cE2+C7tQeqw6qYz/8Z6SQU+8MADRbmOndlv0vMRZSqQNqjK09CqXrU+OBe0R0S9gGq0B+ul7MXb5lznVeA8ca1xDXKNKDmAoD3YP970jyjPGb2b6gToIh1NCpq0Mfd3zh+i1p4K+kV70AYMALYtgZ+mT59elDlnXCN1jlpS3PSwy32ipEvvkzpzSfmZZwjrYb+Vp0yul9LO1KlTizLXB6WWOgGpaOOIiP32268oq7Oa3ml1cqLwzKDnHtdEhJaNCXWGUGLl2anWR17XyvPpwAMPbNkn7kPaQ3m58fMsZXPv0wYsN+vasmVLbNiwwbKLYRiGYRjdB//4MAzDMAyjo+ha2WXGjBnR29srU3ST4uOt5BxkjDef+T7lGZqAddEbhGXeGFZ1ZpA+VPXSa4DjVvXylnYeN6ky0oft1svyXnvtVZRJn+Z6FN2u6lVeSLzVXcc7oKpe2oBSF+lNtsF6Vf4X0tS5XuX1QXqTbZD+Zj2k8FWdVfXS24WyEKlwFThNBS3i2sr9UvWqwEqsl2uYwZBU+vltqZf7jWcDqXCVrj63RU8DBv2inEPvgDr1Um7imqDMkutlEDXlxcHzh1I2qXPS83w+72+1j7k3WC8D/ykb8HPag32NKMtu7dbLMuVanjOUJrMXX53+ch1wTXFPsx7uHdaTpS5KqZz7duvl55Qg+S7tmsfxZgHZhoeH4/7777fsYhiGYRhG98E/PgzDMAzD6Cj848MwDMMwjI6iaxPLjR8/Pvr6+kp6JhONUQuvclGidjZjxoyiTH2TdzgYZU5dh6H+xnsQWSujSy3vhrCPbEO5SFLzpiZZ1zWLdx9YpjsW9W/lvsj7GHTHYl8jykntGDFQgfow3d6o+/OOCe8uVLn5st6DDz64KPM+ALV0VS/njpop5z7/jWu1jjso7zIo91jOUZU7KPvBOw60J+tiG6peatxZy+UdELoOVkVNbFUv9XLuT+7JqntVyjVY3dPhPSBVL+c+r3PeG2Cyrzr2VK7AtMEjjzzSst9V9bK/dP/lXZJ77723KCv3cPaPZ3BE2dWa7THxZ9U8NcF7JUykppJwVtXL/vKeCM87zgv7SltWnamqXs6TitrKevk5XXC5nvjdERFxzz33tHyfoD3pxs+5f+ihh4ry/fff37J/edy8B6mSejbvOdVxn27CzIdhGIZhGB2Ff3wYhmEYhtFRdK2r7aRJk6Knp6dER5P+IUVFV7rskqZof9JDpDoZ4Y4uRqTTSBUrOSWiTM2x76TklZsjx026nNQfnyedFlGmSlUCIdLItCflClKVKslcTsBESYbjUNEN2T8VvVS5+ebIruwj2+CcqYi2jz32WMt+EBxPdm8mdUyaXFGxClw3pEzVusn9VbIL+0fJoU60U46b9osoyzDsI10YVf84X5TGKLXQflXHlYryyjIlVko7au9x3dB++TnKLkpmY//oRsv5Jt1eFe2X808bso+0lZIXlT3pRpwj2nK9cJ5U/9h3RiglVP8yaFvuP5WgrU5iP9qPEliOnsx1qBJMqv1DG3J8PL/Ufsn1sl8sUxbn2lHfUSwzIm2WS5XbNO3WrGvLli3x1FNP2dXWMAzDMIzug398GIZhGIbRUXSt7PKRj3wk+vr6SvSWkj44hHzblnQXI7KRYuc76oYyI0VSrqDHSPZ2UXWRrmJiIFKP6sa8ovhyMiDeRCYdpyg4RQmTsqbN6nodUNIipUmvCNKNihbkWJlwK3sBUE5QS5u0IteHuhVed4uQAlWeJXmNNKGoenossB56QEXo5F+0G+l97gWuYSWHsX85wim9SSi71EkSSFmI0hrXLPdClVcXbcCxkmJnvXW8cdS+jyhLJNxval+pvcBzTdksr0GOlVIG31djVRITPc1ImTPxYx6TGiv3AtcL51JJF0T+nOclZTOuO3Ve0ob0MuG+5xmX14dKUso1wnWnEnEqUOrK0ibPNZbVeakS53G+KadUJYHkWPndwH3/4IMPFu0+/vjjll0MwzAMw+g++MeHYRiGYRgdRdfKLlOmTImenp4S5UMah7Qek+tk7wdCJZAjxUUqkLSUou944zoHnVKBzBQtyLEecMABRZmULilyVU9EmXbjTWaCwXFUgi6OlfQd21u3bp1sW9GppO9IN5KqVLQ/KUJSf7leBtFRUMG9WE+mnRXYX8oJpMUZLEiB9mM9lIgefvjh0jvKq0v1iXNJ+UBJYOxTplNpK5VUTdXF/UPpkGtK1ROhx0f6mxIVZQm1p7mvPvjBDxZlyiPbUhf3tPIoU/JUHjdtxX5kGfLNQC8Rnld16+H+4TqvE3BPeUCxnnwucd3XCbTGPU0plLKqOvuyhM91TqlFSTVqzdNjkdcHmDSPaz7XpQKqMcgYJTfWRZuxbZ7BHGeElo8o/TW9K4eHh2PNmjU7XnZZuHBhHHHEEbHrrrvGHnvsEZ/61Ke2cvFsNBoxf/78GBgYiHHjxsVxxx0Xq1evbqcZwzAMwzBGMNr68bFkyZI4++yz47bbbovFixfH0NBQzJ07t8Q8XHzxxXHJJZfEokWLYvny5dHf3x/HH3986de5YRiGYRjvXmyX7PLss8/GHnvsEUuWLIkPf/jD0Wg0YmBgIObNmxfnnXdeRPw2oMrEiRPjoosuijPOOONN62zKLlOnTo3e3t6SJEJaijf/c+4BghSSorhIJZHeIq1ECokeIGvWrGn5fIQOckWqjM/Q24VUIKlN0mOUeUjjRpRpU9pH3b5n//hjkgF7+Az7RHtElClJSjvKe4WSgQpCROpWeSflv/FmPMdBO3POWBflAJVnJ3uuKImJt/1VUCDanP3geGjX7HGiPFYUNU07K3mQaBVQqAlKOBwT7cM+sS5KJfQ6oD0oBWUqnONTUiqpeuUZx/3GQINKEozQ86280Dgmnl9KpuG+5dxHlM+sOnmuaBvKd00vhQzaJtucnjqUMrgGlaTCPrEfnGOuwaozlWPiHqU9eA6qIGMqiGMOoKakKJ5f9KLh2lZypLITJbqIcmAy2ofvKDsz8BwlvjoB4iLKa49zT3s0v0+3bNkS69evf+u9XZqT3DxQ165dGxs2bIi5c+cWz4wdOzaOPfbYWLZsWcs6Xn/99RgcHCz9ZxiGYRjGyMU2//hoNBpxzjnnxDHHHBMzZ86MiP//i4y/Hpv/Vv9HtXDhwpgwYULxX45XYRiGYRjGyMI2yy5nn312/PM//3MsXbq0oJqWLVsWRx99dDz99NMlL4T/+l//a6xfvz5+8YtfbFXP66+/XqKUBgcHY/LkyTF79uzo7e0t3eAlDck7JKT9M5WkqFFSVKRZSc2RViL9pwJs5dwP9CJQ/SD9Ryqd7Sn6uip9MakyNQ5F+bFe0mykM1VOmvxvjolzpvJ7KBqR8hntkfNAqPHV6YeSKDhW9iOvtTpyB2l8Uv2k4fm5CvqVPZh4417JW8pjhTZTNHXV2Lh/GACJMh2hxkTbqkBKGSrnhXpfrTWCciRp6qp8G+p95eWmgsLRNjxPsjzCNaL6wXnhOcM5rpOnKkuh9CLLeZ1a1cV5oc3ZD46Hz2RJnfuYUkadfigZnRK8Oj9yX/g+pUbaTe1jtqECsFUFlWO9lIxUgEXOsdpLlE7zWmPblIx4Djf37tDQUNx55521ZJfWPkZvgq985Stx7bXXxs0331zSuJq624YNG0o/PjZu3LgVG9LE2LFjSz8wDMMwDMMY2WhLdmk0GvHlL385rrnmmvjlL3+5VbbDKVOmRH9/fyxevLj4bPPmzbFkyZI46qijdkyPDcMwDMN4R6Mt2eWss86KK6+8Mn72s5+Vgu9MmDChuKl+0UUXxcKFC+NHP/pRTJs2LRYsWBA33XRTPPTQQyWKSKHp7XLEEUdEX1+fjF3Pm+O8pMq8HxFl7wTePib1zpvjvHVNuonMjUpJnW9DK5pb2YE0ovLCIJVK+o2UWwZpzByUqwnS/qRoOT56I/B2Od+N0KnK1ThYL2/MKxuQxs20LOdbLW0ybbxjxLZ5g12NgZJURHkdUZJRVCflQtarqHoV3C6iPK8ch5LHaFt6hqjU5mofRpQDepGW5ZpU4+Be4JqilxRRJX1QpqB3lMqrwf3Jc4P9rnsBnvuba5v9UOA+ppcD6e+qPCPcr/Tw4nlCilwFv1L37bgmqsC5pITG4GxqTdEG7Ec+1yj9Ko9ABk7j+DgXSn7jPuK5FFG2A/cV26CMwu8brkEl83BP5/2tzki1L/l9xTOS9uQ5Q9klt82zjH/jmBhk7P7779/xssvll18eERHHHXdc6fMf/ehH8Z/+03+KiIhzzz03Xn311TjrrLPixRdfjCOPPDKuv/76Wj88DMMwDMMY+Wjrx0cdkmTUqFExf/78mD9//rb2yTAMwzCMEYyuze3yoQ99KPr6+krUDm/tkt6lBENvgojyLWh1u53ULW8ck4JjEC+V9jrnCCD9R4qrzvukwkl7kbJTcfojytQlqUpFhfN9vlun7byESH9TiqI8o6hf0s58nmXlhRFRtrkK6lQn9TTf5bjVu7ltrjXSzqrvbJuULqneuuMmBU3qXck/lI+4Btl2lccJ5SNSvIquZ130DKFcoWSX3A9KcCwz+J/yBuHzbJt5ZdSZsS3vc59w3KyHHnLq3YjyOUfZq877lAModTGIWlXbfJ/rjjKR8mhS7zJNR91x830GklPp4bk2eUbVbZtyLT2AVNsqlxidNLhW1LtV73POeG5QHuHapMzz6KOPFmWeu1XXB9gG+9tsY3h4OFavXv3WBxkzDMMwDMNoF/7xYRiGYRhGR9G1ssvs2bOjr6+v5H1C2ou30HmDOudAoIxCGkh5ovB53lymmUijky7PaZDZhvKQISXGz3PekFZtMLdLDrZFqUAF4CGFSTqOEoUKiEN6nhJFRNljiDQmpSTOH59XOV9IU9OulJQiyvIRbajSmbe6sR1RphTZb3rT5DT2tJVKV81x5/eb4E119k95EUWUbUIanrKZkjA5R6SyKdlwfWWJT92yp505x7y5r9Ym1zmp3ty2ym/E+VMeP/RwUflf2F5umxIC95vK6cT1xTXBM4fjpp3YVkR5z7G/HCvPS64J5YGjaHvKQhHl8095W/BMVcH0+DxtU+X1oaQ8OjTQHrQB5QR1Bqt1E1HeAyqnEPvLNcF6+QzXB89Xvpvb41xy7RA8+9gex82zjPOVzxbagWcW13DzneHh4XjooYcsuxiGYRiG0X3wjw/DMAzDMDqKbQqv3gls2bIlhoeHSxQmKTTSk6SCsheAyi9BCk1R7yrQFGk25XUTUaYbFQVFGoy0saLmKPOoFNYRZUpM5WdRHiQqxbTyQMhePryNTalABcchpUwPI1LhpBf5Lmn3iPL4OMf0AqBtGKWXt78poXDcpOqZ0yGiPP8q/w/Hx5vqHBPpUPaPazvLcnyfUgs9UUiXs38qx4/ab3lsSprg/uHnnBeuL86d2nsZ/FuWPZug3SgrqVwYqj3S3RH1AqFRLqTdVNp4nj/M35PzbaggY/RUY3vsK9tjmQHKuBey1EF7UpLknNXJacO1TSlISVj5Ha4jnp18h/ZU7akAizlgJdcL21ZnDvd6DsTYhMoRk702uab4HNc8+87vKHpX8nMVxDF7C/Ed7mM+1zw3qvKNZZj5MAzDMAyjo/CPD8MwDMMwOgr/+DAMwzAMo6PoWlfbOXPmRF9fX0mrpwbNOwC8c8CIfRFlDZuuTNSrWKaWSvdA3oOgnkk3rQyalmX2l+501OCos9HlkZqacheO0FFb2Q+2x4h96u4DNWRq5/nuA3VP2pb3D2hnNZd8l4kM77vvvqKs7vRElO/H8H6Fun/AMVFDVm6mOfEU1yfnjLqzcvWkps96lJ7PNRihE13R9ZxzSV1dud2yf/fee2/L/kWU1xptyHsGXM+8G0AdnneCOAauiZwjis+pMfF9FRWTY6INDjrooKK8evXqUGA/eMeKmjzvP7F/XBO8Q3H//fe37F9EeU3xbgFtzntjXB/cF4ywqVyM89nCNcm9zz6yfxwT1znXhNoXDzzwQKlt9lElquT88a4F71FwDLxbc88997R8PrfHe23K5uoulXIl5t0RFZU3omxbri/WpaKl8n4d7/Lw83zngy7KtDPXc9Mew8PDcc8999jV1jAMwzCM7oN/fBiGYRiG0VF0ravt5s2bY3h4uCSV0C1v+vTpRZm0EhNKRegobqQISZ+TYmfbKhol6UK67OY2+L76nNQcPyc9yb6Smsuul8qVmK6ppGLpeka5ie6FfJ50a05ERBqYY6IrKvvEMdHmpCFJQZMO5fO5L5RaSL3TBqRMaQOOddasWUV5xYoVRTmvCY718MMPL8p33XVXUSadTRr/sMMOa/k8KVZKkDNnziy1TSmKY6WkpaK8ch1xDKRflUyQn1MRL6sStL1ZPZyLLHNy/jhu5Q6qEs5xv/B50v55DHyfkhjPHPadz3OPUKKjFESbs08RZcqcNqDdKLmxH0ykxvX1gQ98oCiriM4R5f3KvaTeYf8oi9OetAHniK7KVe+ocXNMtBm/C3hm0M75XKNsyfOW9mA/aA+uRz7DM5Gf57a57lUbKgI4JVpK39xv6gyIiDjiiCOKMt2NWVfTnbfOPi/arP2kYRiGYRjGDoB/fBiGYRiG0VF0rbfLgQceGL29vfImvoogyWhuEWWqjLIN6Ube9CUdTXqSFBNB+YC33HN/Cd5K5m1l9TxtQPqOU5cTdJG2Iw2pvHyUtwRpUhV5MN+O5m160si0OSlySju8fa+oVBXdtqrvpMLZtvICYL/5PO2cPasoi7DvXDuUtGg3joMUPqli9iMnEqS3EvuuomeSLn/ssce2ud+57/QCUFEuOX/7779/USY9z+eVt0p+h30nTc01wf3K/UN6XnmHZVmVXhm0oTqz6Nmj+p1lRNU2559zSe8O9p3zR/mNa5Z25hzlSJ+UMzk+7lF1VrMflDgo43Lu83lOrzfanGubbbDvHCvniN8LVetcrWEVWZfPcw1yjtXaZD9yfwm+QynoYx/7WFGmDblXKc1QyuZZGVGWldjfVh5G9nYxDMMwDKNr4R8fhmEYhmF0FF0ru8ycOTN6e3tLNBGpIVJJpIVywCvKD/RA4K1dUq5MykXTsG2VfIt0ZO4X5RzS6qTxKTm8//3vL8qUaZTXTablSL+q9zluFUCM1LmiT7P8oJIu5UBJTZDK47uk7ZQHB+nWiLKERhqSFCpty8/ZV84L6+R6ygn1+G/OB+lNrilKFKQ6VSCzquRuXJOkkbnuWC+pX7Xm2QalMcpvEWV6WXm4qIBLDBqmJEjORT6ulLcYbcD1wrXGtul1wHpU4L/cX9qK9XI9Kw82rnPalmszU9hqP3Ct8sxivRwfn+Fa4/mRpSD2K8t/TSi7cS/wc+UNkte5kqhYL/c9+8Gxsl6eIRxbThTK/cN6eW5wntS5xnWgAhDmtjk3lM34PUZ7cP5UMMosKzWRPW245/gOPZGa3zFDQ0OxfPlyyy6GYRiGYXQf/OPDMAzDMIyOomuDjA0NDUWj0SjR+6TpKCWQomLwnogyZUSphrSZ8gBhmXQm3yVIb+U+kioj7UYqj3IAaTPSu6S6aJtMcbEvKtAR+0FakMF7CPaD/SZtn+tSEglpcd5CV5IIg4zxGc5LRPkGfJ1cOew7ZRCuG+Z7oFSVc7twHPwbb9zTg4qSFtcH5TuVGydLPqyL+4H2YRA1SnyHHnpoUWbuDa4PejJwbUaUb9znoFBNqPw9tGcOENgEA6rl/CqcJ+ZhoZ3ZP9qQ8gPtR3nwwAMPLMp33313qW3lscU1SBsyhwjniM/THtwvzLcUUV7P9LLjvPIMoCcXaXUGA+MapM2y9wPHQbmDY+LZp4K/Ua7gulFBvyLKZzj7yDOH8gXb43mgpFCea1lmYxucG/aJY5o9e3ZR5vqiTM1zgu9mOZnXCbhuVbAz5W1Ee6irC7RNRPkMoaTIeptz4SBjhmEYhmF0LfzjwzAMwzCMjqJrvV2mTJkSPT09JSpPeWqQKsvBeEjhkVYnZUSaVUkL6ja2Sl0fUabOeBOZcony5uHtdAYnorxCTwamI4/Qt7xJJXLcrIvBY5QMpVJER5TtQ8qO9KtKja3yFqjnM9RyVlSsykVSp578jFpTSgLjM6xL5QFS9URsTdM2QapYeY2o2/dE1TPqNrwKjKTWPMdU55n8bz7H91nmelTP1OlT3brqPKM8fpQHTUT5nKPN+VydZ+q0l4NoqTlW60B5k1DyoZTKsyvbnO8rL0euc+Wdwe8Vvsu5z98lRx11VMt32EdKkpT4KO/yDOe7Sp6KKNuK32M8j2hnFQSNEh09ZVSgzQj9ncHPm9+zW7ZsiaeeesreLoZhGIZhdB/848MwDMMwjI6ia71d+vv7o6+vryQnkMYixURKK9OypM7oRaDSslNy4A1l0lKK/q5SsEhRUZ5ReVdYLylClkmp87Z9RJm6ZH/ZD1KHHB8/p/eQSque6UnKIpynHLymCdqcc0zPBuVFkXMgkFakPWkDzr269c4x0B686Z+9fEhvkhLmmEjLksJUN/Epb6lcNxHlW++kclVQLa5tekKwHq5Tzn2WU2hn9pGf82Y91yrtTC8k2lnJUBHltUdvAdqc+4RyJtc8Zdz77ruvKB922GFFOcsPbJvzxLVKCpt7gc+T/qZtqvLKcP55JvDc4F7nuDlWlU+Ec8z9GVGeSyUVcKxcR9wzbINzzL3A9RRRnjPl7cIyJQruK3rRqKB3uW16HGVvsyZof7bHda7S19NmOcgYv6OUzbnfOGfsN/cI+0pPuLy/uY7YR47j4IMPLupkPqoqmPkwDMMwDKOj8I8PwzAMwzA6iq6VXV577bXo7e0tUZ2kC0k3keLOgWFIzTGIDuk/piMnxU5qmh4d7Acp0wzS+6S7FG2mbtwTfJfPZ4qQIJ3NvDKksNXtdtqWbVBCyWmvVS4ZUvpsjzag/VWeBdKFOcgYaUxKHFwHpDBpA/aVtCVpXI47y2y0G21F6l3l3OHNcPZJBYjLgYCUrVgv32G9vElP27A9roPs7aLoZZUjhZ+rfC7KIyx7VikPHq5JjlV5N3EMHB/nK1PttDOpaj7HdadkWbanvIpIi0eUA1WpoIdcq5wXnmvKK0VJNhHlcStPCNbL/rEfymYMlJZltkMOOaQoqzxJlHloA54VXPM8f/i9kKVeNX/Khjzj2A/OMdep2jsR5X3C+eD3yvTp04syx8R6VS4fdd0gojw3yluv2ScHGTMMwzAMo2vhHx+GYRiGYXQUXSu7jB07Nvr6+kq0pbrFXxUASXk2KFqcFBVpN95C57ukFPMtYb6/atWqlm2QplJBgQjS4nw3t62kE0UvK4mJkgP7TcotU+F77bVXUVbpnElnq7TxKsU3vTNyQBz2UXlhkMKmdwzfZb2kltm/THdzrCqNO+eJ3lf77LNPUSaVyuBLXB/Z5uwvg9Lx5jnHSlmDVCqlMfabbWeJjzS8onhVPgrWy/mmpxPzqzAPSkR5TdXJ70EqnP2gPUmvs72cZ4Tj5lpQe0YFMGQ/VLr7PN/0YKB9lGzJftCDjZIsx6DKEWXbPvLII0V5xowZRVl5VXDdMecR1y8lpXwO8hygnVWgSeaQ4rxyDOwTz9csJ6sgXmrueQbwc8oY3N88N3PbPCv4TpV3Tqv2+Ay/N5VUG1FeIyrfU3NNtROz1MyHYRiGYRgdhX98GIZhGIbRUXSt7PLSSy9Fb2+vzAdCKpVl0lv5HYI0U51b2qyX7VHGINWY/0YvE3rI8JYxZRpSsaTB2G/ShdnLh/SkSqmsgj3xxjepZn7Om+PZ84J0MW2rqD0+Q9qZ9LCSpyhXRJTnifStCm7EcXNMbI80Z1XQNM4f54PzRKqSdCbXAW/uk0Zn/VlmU/IDPbnYD7bB9ZhziDRB23BtRpTlHPaXgb4yjdwE5QTuPdqP5WYwoyYoF9KGXIOUKFTeIZWHiX2izXLfc3DDJniGqFwm3Ie0E8+uLH3sv//+RZkSACVJzkWWbZrg+ucZxbWS21Z5aVQgQI6J6/TBBx8sytwXVVK2ktN4JvAdjpv7TXkN8vN8plIa4tpmf3nGcUy0J+VFPv/MM88U5RzQTs0f97ryjOMz3LsqcGAO3EjvTn7HtQqyZ28XwzAMwzC6Fv7xYRiGYRhGR+EfH4ZhGIZhdBRde+fj5Zdfjp6eHhkNlFqjcqmM0PotNV5qq9TNqJVR11Pugfm+CTVXFc2P+ppKWEdQ+6PGnaMvKnc66vPU51SipYcffrgoUyOn/pmjELIvvA+idEvan2PiPQ0+Qy0210lNkrbleqHOyvWhxsByVdIk1sV1wXEwOiHdAJWrINczx5oTjfHuA+dYRdtkvewH+6dcc6knR+iEWLwTo5Lrcc1z3fH+gXLTjSivCz7Hz9kn2lAl2qPNeB8ju3Xz/oKKzqoSPLJPXPNcv9xX2eb5LkQrcF44bt7B4H0Hnld0+2Ryz4iyfZTrMp/hXlBrUCUhzOPmGcs55lzw7FN33Ghn5Y6bwXnifKvkorzjwztIvAvCdaeiv0aUbUu7KTda2rkqUV+rz3kfKaI8VnXX0nc+DMMwDMPoevjHh2EYhmEYHUXXyi5jxoyJ3t7eEkXLyJSkpUhj5URvpPPoeqmSgKlom6SYSHWRKst0lXIFY72kVpXExP7xXUXRRpSlCdKvpI7ZX9qNfSVdTupQ0aS5XvaDzyk3YdLROWlcE6T26J4WUaaRVTRYUsXKDZBUM9cNx5ZdbZUbNNsgJUxJivZU4+Nay3Q03RxJs3L/sH90GyX1y7HyXVLTlOJye1xryi1Z2YNzxzXIergvIsp0NNcn17Nqg3uJZa55zgtp94iy3bhH2Se+w3XO+aNtaHMmUcsyG9cXzw2eCZxvQsnGdaSEiLKEw3c4Pq5b2pD94/5mG9xjeQxc57QB7a9kZuWKqmSyPG6uWzWXnAsVKZrzzedpj7zO+RxtUie6MMetnue4ab/8XCv32oj/P9+531Uw82EYhmEYRkfhHx+GYRiGYXQUXSu79Pf3R19fn6TZ6sgjEWVqW92aJ83HMqUMRuNTN4xzVDr2RXnaMPkQqXCV8In2UNEhMzgO9pF0HPtE26oor3w3RwK86667ijKTnPEdSh9smzS1SnJGujtHnczRGJtQlCltQ9qT/aPER9mKniER5eiSrFfd6mc/uE75OaUg9ilH06VMxPenT59elLn+lexIjweCdOsBBxxQ+pvySmJStqlTpxZlUsIq6aGKWJmlOEZf5NzTBty7fJ8eXuvWrWtZD/uRba48SBhVluuFc8/2mJCNEjLXf5Y2uS7UPuYzKrkh1w29QZS3XURZuqKtaB9Knio5KM8+9rUquqqSH9ge9x7b4JrgXuJeUEk/I8rzTXsq2YX1cv6UBxr3S5ZV1RnC5ygx8RxVsihto86+iPJ64bgpETbbHh4e3upcVDDzYRiGYRhGR+EfH4ZhGIZhdBRdK7ts2rQpent7S9SXSipEOSDTVby5riQc0nekqEh7Tps2rWV76mZ7hvK8UImMSOGTsiMFRoo8U2XqRjUpO1Le9A5QXjcs890su/B9FaiNc8m+qsR+tB/tQco/okwXk1ZXdLRKwEcKWckgeb5Jm9LDgvbgO8qjQElgDz30UFHOchOlkLVr1xblFStWFGXSsqRZOT4VqIh9evzxx0ttc25YF2l8RX+rYEgq8Vf2QODeVTf8VaI+FfCKqEpyxjXCdcs1pQJ9sT1KMyrIVQ6mpwJSUXKgVxK9mPguJSJKMGwvn6nKQ0+dGyrwFOvl87RZlpN5VijPJZ7JDJbGfnA9sm2eLbltSofcSww8qLzZuO95hlBCVkEHI8rnM9ckzz9+p9EGlEF4nnCtcS1zH0Xoc4Djbp4BHQsytnDhwhg1alTMmzev+KzRaMT8+fNjYGAgxo0bF8cdd1ysXr16e5oxDMMwDGMEYZt/fCxfvjyuuOKKki96RMTFF18cl1xySSxatCiWL18e/f39cfzxx8uU2oZhGIZhvLuwTbLLb37zm/j85z8fP/zhD+N//I//UXzeaDTi0ksvjW9961vx6U9/OiIi/vZv/zYmTpwYV155ZZxxxhm123jppZeit7d3KzmhCVJuKlBURJmmUzeGST+RvuPnpLFUYJ1MmZISU/Qk+6QCBKkgXJSRsqcN6THaUAXhIlgX6Vf2g2PI+TbYHt+hzenNQzqT/Sa9SBqdVGoOQkRvAf6NNCY9DUh5k24kTa08lbL96LWgbsOTwnzkkUeKMmU2Uqxc21wf2eOEFDup9JUrVxZlrk/2j+tUSU+ULnIwPY6JNuH8qTXBtrl3KWFxDVJGjShT1cyloWROtsH9w89VbiJS5BE6TxLHyrkk5U3QfqyzKqjcqlWrWtbLvURPM+63bMMmKA1wL+S8UewLJSYloXEuKMNyT9JmbPuBBx4otc2+q+Bx3Lvcr5xLzj37xLXN5/O/uTeUZMpzlHKfkpjYj3yeq/xlLLMfPDeUlJfntQnKNxHl/aq8ppr9UPJlK2wT83H22WfHJz7xifjYxz5W+nzt2rWxYcOGmDt3bvHZ2LFj49hjj41ly5a1rOv111+PwcHB0n+GYRiGYYxctM18XHXVVbFixYq48847t/pb8/8M+H8jzX/z/zaJhQsXxne+8512u2EYhmEYxjsUbf34WL9+fXzta1+L66+/fisPByJTL41GQ9IxF1xwQZxzzjnFvwcHB2Py5MkxMDAQfX19JbpK3bLOeU0IUn4qwFO+3dsEaURSjYq+zlC5EmgL0m7sE6UM9pv1qIBauQ1181zleFDBjfi88nyJ0IGASNNxrMrDSFHhtFnOQ8AfvqyLa4TSDJk2zjEpUNqSY8hUOMdK+pa24tqh1MJ5UanhjzvuuKK8dOnSUtvf/OY3izLlnCVLlhRlzh8pdhUoj+OhfJnzjHB9quBs9JChLKRy5VA+4DyqIGgROngT1wvrYv4RSnl8l5JPPico2XFNqT7SHirwFueb7XE9RkTsv//+RZneDJRB+D69m9ieCrbFNUgpKELnZ+E5wP4qryKeZbQN10H28mEfue7U+GhPdQar85X1RJTPENqHdlaekxwfbaPk+CyVcA1zrFzP2VZNqLEqWTt/V6v+so9Ne7Tj7dLWj48VK1bExo0bY/bs2aVGb7755li0aFHhCrhhw4aSHrdx48at2JAmxo4dW/njwTAMwzCMkYW27nx89KMfjXvvvTdWrVpV/Ddnzpz4/Oc/H6tWrYqpU6dGf39/LF68uHhn8+bNsWTJkjjqqKN2eOcNwzAMw3jnoS3mY9ddd42ZM2eWPtt5551jt912Kz6fN29eLFiwIKZNmxbTpk2LBQsWxPjx4+PUU09tq2OvvfZa9PX1lWhBlXactFCmq0hLKelE5RhQKY5JL/K2fvbMUXKCum1O6ovtqdTfvLGdqXCVo4Cfk8rjDWf2T6V057uZaiNtSnqYNKaiRlkvP+f4lJQToYOo1XH15hwpeUV5WUWUx03mj2tE5Skhdc51w7VJSj3bnOO++eabizLlFUpUSp5SAceU50pEeU1SxlL5QThuzqWShVhnljm5B9T6UvQ+26BXCsddJX0QZG+VBxzbYE4aetEoGSrPN8f96KOPFmUlg3B9sKzkRVL4VVIX9yv3A88TSt9qTXBelSSb+8Kxqu8AdT7zeY6V+zB7fXBuuFbZHuvlHDOAG+uh/Yh8trANFZSO64Vj5fiUDEXZkGdOrkt9RzU/f8tklzo499xz49VXX42zzjorXnzxxTjyyCPj+uuvly6zhmEYhmG8u7DdPz5uuumm0r9HjRoV8+fPj/nz529v1YZhGIZhjECMaqgIJG8TBgcHY8KECfHhD384+vr6StQXKR9+zhu4OcgY6UkGISLNRFpdpbInXdvqlm+rtvkcKVDeFie9pnJkkDokVUz6LdN3tBX7pehGUvrMyUGQNiZVT3oxojw3pBhJ5ymKnTQ36XbaT1G0EeWxrl+/vigzEi/7oeab/ab3CAOJZXqSEgJtom6bkyZlG3yGa/uII44oyll+YN6Xk08+uSj/0z/9U8u6aFva/PDDDy/KXF8qdXdEeS1w3alcH6TLaRuuQcpClP64byPKe4P2pEcN9xjHoSQbpoP48Ic/XJRzqnAlMzDIFd/huJWHBSVP7lXmKYoo70VKEdzTKljdhz70oaLMYG4qp1A+W1TgQM4l1zMlKc4l6+FcKGkmorzWlKeIyqGjgqPxvOMaykEE1XmrJHl+N3B8nAvOMfck11BEeY/y3KGiwPNLnWu0OdcNbZDPFq5b2oDScvOdoaGhuOmmm2LTpk1bfR9mOKutYRiGYRgdhX98GIZhGIbRUezwC6c7Ci+88EL09vbKwGKkq0hvZcqINBHBtOOk6eqkgibFSjkmp3dnXa3SD+eyygNBio+3pokcAIl0rwqWpmQsUpuUiFQgLD6T2yPNR7qRYyI1SvpbeQcor6dcF+lGyhJ8huMmTci5ZHukjfNteBWvht4gzFVB23AdUb5hTJ0HH3ywKGf54Q//8A+L8ve+972iTPqW86pyQtBOKgBenm/KmVyHrJfj4zOcI1Lh/Jz1ZM8LlXeC88TPVWAl1VdGZs6eF4o+5/ypIH1sj+ca1x3HljOD8x3uGbatgkORkifVzzVB+Sd7XtA+/BvXGm2rZBTuXe4lrrsc6EsFyaJcota5CgZGG3BslI4itGzGvqvAkpwLFRxNBfiLKO85fmdQNlOSXZ3cLJzHHGSM+5Jtc49tS5AxMx+GYRiGYXQU/vFhGIZhGEZH0bWyy7hx46Kvr69Eu6kgV6SxclZcSioqLj3fJy1FGl3l52C5qm32XckGKreIkkpUsLMM9l2leq/zDGlBdeM9okznsS7laUOaj9IH55j9UJ5HuS+Kjt5nn31ajolzoW7P8/PsacOxsh8qp43KJ8J6KNOQeuVN84hyrhdSqKSgiY9+9KNFmZIg+0R6nnshy0uUxNR65hyTIidVT5upfEs5XpAKIkgPKq47rgmV80XlEcqSLvefktz4DteRCkBIGzD4WJYu8vw3wXFTDqANlAzF8XBfZBqe0twHP/jBosw1zDXPenlGsm0lp+QggsrTkHOmZJ46aTy4d/J8s23ah88p+YLvsh88l9R5l8Hx8Tm1hmknFRCQc5SlLtbFM6hVoEnLLoZhGIZhdC3848MwDMMwjI7CPz4MwzAMw+gouvbOR1M7olZJ90JqV9QaszscNSq6NeXnmqAeR+2LdxFYJzXJrE9m7awJleSJGpyKdso2VJTQiLL+p7RE2pD94B0AdfdEacgRZTuzv9SpOT7qobTZ9OnTi/KaNWuK8pQpU4pydndlHxmlVCU8mzx5clHmXQTanFo9kXV33rvhnQW6DDOS4MqVK4sy9eEDDjigKNN+hx56aFFm8riIiE996lNFmREQ6X7385//vCgzGRldZbluGFVTuRlGlPcG186RRx7Zsg3lZsq5Y7+5zvl8RDkSKtc55/7xxx8vyrzTQr1duQhXaeFcF/wbz5CpU6cWZbpKH3bYYW/aNm2eXYz33XffokxXSN45UPeTlLsk+0qbs/6IKCUX5TqaNWtWUVb3aRjpU7k3M5xAXmuESnbIfcj1oVx71Xme74ioyM0M58A5IzhWFXW1KoSAuhOjwHFwbao7ZxxbPlPV3m91zyZ/F1TBzIdhGIZhGB2Ff3wYhmEYhtFRdG1iucMPPzx6e3tLlBEpUEbwJE1EujaiTFGR7iIVSNqYNKSKIqiiaObIj6QC+Q7pXtZL2pNUHik+0qGsk3RmRJnKZaQ+0rXsB8dBFz9KXZRauGwy1cb54PuUH0iHkgpkvZSSSMXSzvvvv3+pbb7PKLaTJk0qyqSmSSmSAqVtuKZIhzJxXURZxmIiO0oOShLhuuOaYPnggw8uyhxbhE68xzFRelLJ+Zg4TyXZyu67yoWRc8Z1wLXKPrE9ymFcT9m9mTSwcsVnP2h/JQsR3GM5qqxyY64TuZbyFNcmbUm5KCca437lvuS4KS/yLGKf6DbLffGRj3ykKFNaidBu4Uw4qKJ7quRudNfOtD/Bc437rU5CSrbN5ynTcN3lCKdcC5xj1sV1pGRqlXCRds3J/PgOzyyedzxP6CbPd/mMiqabz3PVX55NzWeGh4fj4YcfdmI5wzAMwzC6D/7xYRiGYRhGR9G13i4vv/xy9Pb2lqhK0twq4mhOyEPKSEWzpNzBRF6kG0lXsQ3Si/mmM6lORTGy7yrSISkx5R1AOjlCe8Xcc889RXnatGlFmTR+VfTSVs9QAoso06+kQDNl3qqvtCftzPmmN0hum9SoStpHGUtF2yRdy35wvnIUTq4Rzg09FWgb1kuqmeOjPHXvvfcW5Xwbnu+zj5mub4Lyyh133FGUOV9KAsteH8qLgNEvWS+fVwkiOa+ce+6d/DeV8JF7g33i2ua647uUPzl3+R1SzMpjjnIh6X0VLZPeRtnbRUWcZT94nihvHIISJvua5WR6bFGaYORgtkevIBWBmGWuiUzdq8jUXDtKDuOe5L7nfFOu4BmQn1OyEt/hWcR1y7EqD858G4JnOL9nWBf32Jw5c4ryXXfdVZQp5REqInNEeayc+1ayjb1dDMMwDMPoWvjHh2EYhmEYHUXXyi677LLLVgGFKBOQ8uEN4ypqlDIIP2d53bp1RZm0GWUT3kRmme9GlGl1lfCJ75Na5eek3NhX9i/biu8QygakBdVNadK1pPhyEiSOm1TigQceWJRJhZOWVQGeCFKV2QOBILVKepKeApQ1OEcMAEaZht5C2duFdCjbJgVKjwJS71k+aoI055133lmUc7IvgnNM6Ynri2XKPARtQztzDLkvtKEK+EbvE3oFURrjuiONnr3ZVBIxrjsGZyOlzHXHuaOcRs+cLDdRpmDfuU9YlwpISDny4YcfLsqUXXKgO84BbaIC4qnAVgzkx/Fx7rmGIsrnF/cD54wSDD25aGe+y/FQQsnJOmkrlTCN41B7kmcLZROeDdnrRiV041wq+Uedo9z3fD5/j6mEbgTr5XcR1yDtwTXB/Zk96T70oQ8VZX7v8p3muIeGhkrfH1Uw82EYhmEYRkfhHx+GYRiGYXQUXRtkbNasWdHb21uiM0mVkY5Tt6kjyjQY6SfeyiXVRnPwXT6jgmLlm8SkwShrqEBhpAv5jMpPoPoaUaYMSTeSFifdyFwT9913X1FmACSOm14A+XY0x0SqktSoonWVHEYpQc1X/rcKiMP+qoBxpEDZD9ovt826uA6rJJJWfV2xYkVRZrCn1atXF+UsAVDGIpWe83I0QepW5UdhG/w8rzW2wXw3Km8Fy1wTbINzxHWaZQXleca55DsqdweDfilJJEubbJt1cUzsB8enaH+VpyqvIRUAkbQ61zD7SilJ2YxljieivFarpN9Wn5PSz/W26neWfFSAOj7H9cnx0TNHebNRwsrnGueP46Y9OJdcO/we45mqcqrk/c32KMVyvXCd83OVV4ZrlmPLEjClFubVov2bdQ0PD8dDDz3kIGOGYRiGYXQf/OPDMAzDMIyOomu9XbZs2RKjRo2SdCbLpKtyUCVSRopWVEGBCFJopPJU3PsIna6dbZAS441m1kvPBN6Ar2qb9CQpRpZJf1Myom3Yb1K6pOpZzqgTdEalWOe7Kg9NTi+tJI46XgAE55vyASnhTMuS7qWdOSa2p1KKk65U9WQ6Wskut99+e1HmnlEp1pW3UZU6SzlT2ZNzyfGpfCyc16p+KG8n1V9Fl9MGnIuqVOa87U/k4HNvBhUwrEquU8H/2EfaOa+XJpSUoNZmBs/COoH56PFGqDMg25ySMM9LFbCM3wcsc92x35zT/F3CMVECZR9pD56LnFdKIixT0s1B5ZS8qIJRqgBsykOINqd0FFGWvCmb8bmm15pal61g5sMwDMMwjI7CPz4MwzAMw+goulZ2aTQa0Wg0pAzCG7+8qZ4DnCjvDkoIihKmFEFam/QWZR3miInQOQYI3nwmvcVxkw4lVcbAPFneIK1Imo7Ba+ilwFwf6rY4oSSRiLJ9KInR40dJTMpThlBrogqkdVkm7UzKkH3iM5yjKkmJfec6YF0quNGsWbOKMvMyEFw3EWX5gVQuJQAG9OK8qPWlvL2ytwtR9bcmaFtS+tyrHIPyIMj95XMcE8vKW4xl1lkVZEytcyWXcE8qO9M2XENVshef45rkWq2TG0Tlt8ltq3w8fE6lkKeMy73Adcp6ss0pKao+cXz0MuH+pp3YJ+5VBq/MbfB7gv2lvMW1s2HDhpbvsh9Kzo8ozwe9TLju+A7lI35HKWmM66MqaCTPHY5pW5xmzXwYhmEYhtFR+MeHYRiGYRgdRdcGGZs+fXr09vaWqDJKHwwGw4BXmfIkvalube+3334t6121alVRZipuyhWk0HJwFlLVpECZ60AFGeO4eeuadVLmqQoExClWgWXUjWi2x/6Rvss5EEihqjGpwF2k+BSlS7o75zlQt7w5DtKn7CvHQe8YPq8Cl1WNg33iOFTwqzpjyJKU8gpjG2ouWCZtrG7Y5z3GtVBnHO3KBCp4VW6D9lG391XqcBUMTO2L3EdS1aT9VR4nQuUMqfJAoN2Ud4gaB9+tkwMq7+/tGYdag3xGSZ657ypYnfJiUmtKBQfMX420LfMTUX7gmqK3C+Ucnjl8l/VXeUypdbvXXnsVZc73jTfeWJTrXEOgVBVRlnCUZ2PT5sPDw7F69WoHGTMMwzAMo/vgHx+GYRiGYXQUXSu79Pf3R09PT+lmLz1c6Okxbdq0opyD0jDtucqnQA8Q0m6kxCjtMH8FJYBMw5PyU/kbmG5dBbMipU5KUeWQiNA5F0hjqpwEpE9JuaoU9/lz5fHApVaVt6IJRanz80xPcg5UbhHals9kT4pW/aYtc0AdRTWrG+0E54XvKrmCtHhEOSgR66J0wvWoAkqpMqnefBue65nPcdxqDStZiWub42Y9EeX55vyxDfapTr4aljnWPG7ltaM8WZRXEetVNs97hO+o/qrcT8pOLHNfKWkmonw+qLlnn5T8qdZHPteUh1memyZUKnuOSeXGyYG+uH84btpTnVM8Ix988MGirDxXclBE9TXN/lIe4Z559NFHizI9cGibqkCRlGHYR9qwOb7h4eF47LHHLLsYhmEYhtF98I8PwzAMwzA6iq6VXWbOnBm9vb0lupBUlPIYyVSPouEJvsOALAxYNnPmzKJMupbeMVW3oxVdr6hRok6eiirZRQVpUjfmVVCsOh40uS8qsA/nhc+rOVKSTQbpTQYy4zjYhrKTQtVNfCWpqGBRKhCWkkRUPqKI8rrlPmFdyvOijsygaPH8bxWAinZTc8x62D/S+QwIFVGWn2gryk0qn1HV/mnVjzy/KscG36kjaak8Qtwj2eNHpbWv8tZoBeVlwrnL8qLyGFISjpJdlKcf7VE1BkoWShImOD4VbIvI54HyKqqzf1TwRJXiviqwG89n7gfOBQOksS4loe27775FOX8PKVmqVV6foaGhuPPOOy27GIZhGIbRffCPD8MwDMMwOgr/+DAMwzAMo6Po2sRyY8eOjb6+vlJyNxWVTiVNiijrYIwAx0hv1K723nvvonzooYcWZRWtUWm0ETqSJp9TWrrSv9UdkazLqvsBSkNV0UuVdktkN0C6JfNODKGi9HEu+LnS5LMOTxfsfC+iCdpA6deEcpXN9lDRG1WU0TqunqpPWbut4+aodHHlSsx+cC3nNcR3aH8mElQ6t4qgq6Lb5v2t+pVdFZugndQdK3UPKO8xtS/5jpp7rk0V8VXdt8r/VueRigxb546IupsUofcu7x+oua9yo231TFXf1dmpIrASvDeh+pHPD9pKuTrXOfPVGqw6W/gOXX65HziOyZMnF2V1V+VnP/tZUeZ5ks9UrmH2q9Udsqq5yzDzYRiGYRhGR+EfH4ZhGIZhdBRdK7vssssu0dfXV6KPGMGNEgoprUxHKzcvRe+TblT0NZ9h25meVEm6FF2oXCRVNETSi5k6pB1UFEnl9qbGrRJd5aR2lFrYD9KQiobnmFSUVhU1MoMyA/tYh2pWUUJJf1bRsoSKtsk+sa913G5zNFYVHZRQLoxKhmKd7B8p64jy+BgtmGtEuRpyfbEe9lXR3RFleyr7KDmHz3B9qSRleY+x7VZuhxHlcZMip5uoipTKcee1pvaAiubKPtVx/+VZlGU20u1q7gnlLl4nGmuWjjg3/Btdrrk+1RzRnqxHuU9HVCe0bEKFGeC76runar4ZNoBzqVzBVf84dyeccEJR5jxmaZNg1NdWUpAKN9AKZj4MwzAMw+go/OPDMAzDMIyOomsjnH7605+O0aNHS+8ARZ1X0T5KdiHNpCL+EZQSlBQRoRObKblDPcN62/ViidCeFOq2v/JAYFIiJQ3kvihZSs2fmm9CeULkv9VJGqfknzp2yje7SYeyPXW7nWvn+eefL8rKQ0L1L0JHXFRymvKoURFcKT1VefmoPnEN09OMdSnPHj5Db5oIfbte7Qe1Jwk139mDRq1VtaaqJNom2G9GiKzyOCE4T2of15FYlfdORPncUZGK65xNyoNQSUEROtke26tKyNcE5Qe1JvK7tEMdrylie86ljDqRZDkvlKSqpPpW9USUzyYVLbg5T2+88UZce+21jnBqGIZhGEb3wT8+DMMwDMPoKLrW22V4eDh6enrkLXnloZLpSEXLqgBdlFR4s5q0nvKiqKLpSGcrjxpSYny3TkCbqsRyyj7K64aUHT1USL9VeV7wtjltpWQsgvWqcSsPpojyXLJMmpVQFK1KCqjkjQgdjIzBl5T3BG2oAqVV0dF8h15WivJWEibnnhS+8qjIdRHKi0lJbmofKlknokwpq/2qZFk+T0lRBX7KqBuMrNXzKgEZ1xrXbLa5kjv4Th1JVwWh4/N533J/024q4BvbUB5QdQOcqUBtnEt17hIqkSZRFbiR3koMqMa21ZlKr02uc46nyrtJJetUduM+Vt9XKnBchE7K2aofb6m3y1NPPRVf+MIXYrfddovx48fHYYcdFitWrCj+3mg0Yv78+TEwMBDjxo2L4447LlavXt1uM4ZhGIZhjFC09ePjxRdfjKOPPjpGjx4dP//5z+P++++PP/uzPytdILv44ovjkksuiUWLFsXy5cujv78/jj/++K0uihmGYRiG8e5EW94u559/ftx6661xyy23tPx7o9GIgYGBmDdvXpx33nkR8VsaauLEiXHRRRfFGWec8aZtNL1d/vN//s8xZsyYEo2lKJ26QWnqBIAhrcT2KD8oKi/Td+qmtaIOVb6HOkG1cp9U3gnWm2+xN6FyuKhAaZmeVAGNOA4lxyi6sK4nCuvibWsV1EndVKf96jwfUaZQVfAsfs7nFSWvAoPVDYDEerk+FS2uclBQ3qAtI8r7SsmcyqOGoG3Vvs3rn+9wvSgpiHZiX2lP5WWS26YdaEO1hulVR88Z/o+ZWgfZrrSn8nypCpil6m2iVd6OJpTkqrzW3ve+97V8hmOlLavWE88snkFqr6v5U3ZmnXmv8991PJf4vJJHCCWD57r4P/tKrlJnixoD285BxpQsRTTb2Lx5c1x99dU73tvl2muvjTlz5sTJJ58ce+yxR8yaNSt++MMfFn9fu3ZtbNiwIebOnVt8Nnbs2Dj22GNj2bJlLet8/fXXY3BwsPSfYRiGYRgjF239+Hjsscfi8ssvj2nTpsW//uu/xplnnhlf/epX4+/+7u8iImLDhg0RETFx4sTSexMnTiz+lrFw4cKYMGFC8R/DqRuGYRiGMfLQlrfLli1bYs6cObFgwYKIiJg1a1asXr06Lr/88vjiF79YPNcq6JSipi644II455xzin8PDg7G5MmTY3BwMEaPHi1vxtehcSPKN//ryAyKjlb5MtSN8twvtq1S1iuqrMrLoYkXXnih9G9SZ2yPVFudm+d8l3WSilW2iShTvyrvhJoX1Sd18zu3oejXqiBlrfr03HPPFWXKb/ld2oceLnWkMpU+neNRN+kjynZW8iTfUbYluP6Z0yF7N9UJ9MU+ce7ZBudLeZdRxsj10p7Kg0TldiGYR0PJl7kNgvZg35U3iJqLKqqeNmzHwyCibEN6XigPu7zH6vSXe4GyEutSHhZV3i5K5lFtcL4pdSnpVslTVf1i26qNOuclpa4c0E5JVOpsV56hSmIiqmQWFVivOaa3zNtlzz33jIMOOqj02fTp0+OJJ56IiIj+/v6IiK1Yjo0bN27FhjQxduzYeM973lP6zzAMwzCMkYu2fnwcffTR8dBDD5U+W7NmTeyzzz4RETFlypTo7++PxYsXF3/fvHlzLFmyJI466qgd0F3DMAzDMN7paEt2+frXvx5HHXVULFiwID7zmc/EHXfcEVdccUVcccUVEfFbamjevHmxYMGCmDZtWkybNi0WLFgQ48ePj1NPPbWtjvX09ERPT0+JZlN0X1XOCxWgSN2GV0GgSCcpGSTTWCr3APtEpkcFxSI9qdKOZwpZ0dyk89T4VMx/jpX9y7KXksfUbXOVc4RQdB9lkNy2ol95W5xtk27kfNHTg7bJdlL5glQQqDpUs5JasodDnb2hZAKV54V0MtdplllUcDXlkaG8ItgG62G/83yrXBzK04DldmXYKtTxUqkTMJH9VnR5fp9tqBwnrJdSi5LrGDgrSxFKNlNnjpKylT3qBCOM0F5QyrtMya1q3VTZnLbluuX7PAOUbKykj3yeK69PnlnKy41Q/VBnfoT2vuO6aI5DtdsKbf34OOKII+InP/lJXHDBBfHd7343pkyZEpdeeml8/vOfL54599xz49VXX42zzjorXnzxxTjyyCPj+uuvLx3ghmEYhmG8e9F2ePWTTjopTjrpJPn3UaNGxfz582P+/Pnb0y/DMAzDMEYouja3S29vb/T19dUK4FJ1O1rRofycUgRv9fMmMWk2lT+B3iq5jypXiLqlzcA8pDDVzf1Mw5POY38Vja9oN9ZLuo8UX1XgLc6Nym+jPBNoT+Vlkr0f6qwXegap2+m0k5IDMiXPf6vb/sqrRUHlAMnjVPJDHQmGc087005VOW2UxEEbKolDrQll57qeF4rOpqeBkkfq5o1S0pCSZVmvkqGUlJDlY7WOuGfUOaWCMrL861//umX/IsoehOyX8kThXlCBA+tKLUo6VOdJnXrU81niW7duXVGmAwVtwLmg15Q6l1QwvbzHlMzNs0nNhTrb67atzil1laAunNXWMAzDMIyOwj8+DMMwDMPoKLpWdnn11VdjaGioRCspWpZUbFUegiyLNEGKkXQoaSkV1Iz0Vk7bzr6wDV6+VTeieYuZbdTNeaHSSisJR4FjUkGEMm2pcpwo7xMloVVJHE3kW+G0D+lhSh/sEwOIcUysl1QxbZbpSJW3Rd2Sr3PbX3lLZBlDBbZSMhbr4jpicDTa49lnny3KOQCSktZo/1tvvbXl+9OmTSvKXAdcd7RrbrsOdaykBbW/1XxVST51grxlGr8J7km1p/P+Zt9pK7ahpCQ+ozy8spxJPPPMM0WZnjNqf7N/yhuHqDqj6pxf7eYyIfg8z+yIiD322KPlc3VkR65bJQ+ynAOR8X3Wy3ONc0lvJc6lOmfUHo4o7783y7dVJcFnmPkwDMMwDKOj8I8PwzAMwzA6iq6VXUaPHh2jR48uUcUqd0NVKmL+WwXY4q1kUtbKW0XdLs8gLUXvFXVTne0pil3dzM6UsJI7VPr6OkGSlKyQodpQ/VO33il9cI6rMh9zjuukvaY0oAImsU/KwyGibGeVw4JQkooKdMRnqvKrsG11C115dCivrKrgaup9JTPstddeRVnl91CB3fI6599U/iTl4aJsoOjofLYoKZBtq3ND5RBRZ0BeazxD1HmixqE8UQjlgRahvVrqBEjjuFXwN3X+RGgvIUJR/ypYo5JC8xypwG5qTalAlmqOlVSY2+AZp7wf1Zmv6lSB/yJ0jqFWsl4dD76i/dpPGoZhGIZh7AD4x4dhGIZhGB2Ff3wYhmEYhtFRdO2dj2aEU6WjK226KhEU9U2lk1IfrnJtrAN154B3FuhKzHGocStXQbq85b/xHaWF0wbKnYplvpvd1qrcE1v1Sbl9ci7Zb85jnpfsHteE0iJVBFB+To2bEXB5jydCR01USQKVaxzd56jv0gb5zo3S3lXkU5XgjrZVc5fvz/D9DRs2tHyf7plr164tylw7J554YlGmnVSSsvxvNT7ajS7DnD+li6t9FFGeg0mTJhVljpX3XtTeU/cMqqJG1rnDwbraTdzGdZfvm/A+Dseh2lBuu3WS9tFmEWXXeHV2qkSOKtEhn1HJISNCJjlVbvW0f9XdoSa4VvK9FeW2y/b4vjpTd9ttt6LMueMcVUUwVt+JzfEpN/NWMPNhGIZhGEZH4R8fhmEYhmF0FKMadbivDmJwcDAmTJgQp5xySowZM0ZG6VMUd6ajFe3MeklzK3dGfq5cqDI9SbqK77BPyt1JudS2S59GaNq6jrulSprE8VQlclLyinIro81VYrKqKHq0j5KG6rj5Khe2qoRUynVWQdHtal6UW3ZGnaRZyoVduTIqCSyinHCLbrR0YWcbTz/9dMt3OfcHHXRQyz5lyprrUPWXz9RJlMi1pvZ6boN25rpT/SMUPa/WQUR5HDy/6rh6KqlKuT3ntca5VHK0WqsqbECdZHC579nd/M2g5Hklt2bZUc2Nekad80rKVtGd8994ltX5+laSFPvEOcrfoWr/Kdnluuuui02bNpVksVYw82EYhmEYRkfhHx+GYRiGYXQUXevt0gRppZxsp4kqyo7UEG9p84YzPQ3UDXPSYHyGFHSmwFSCMEXBqcRMdWjxDEX5qSixTCjGqJ8cH8dNSjdLDIrSVDQrbUtvFY6BN89JC2YaXskrSv5R861kiSoqvCoRWCuwbRUFVa2V3DZtpWhZ1R7nknOhaO1MOXMODj744KL8+OOPF+WnnnqqKNOee+65Z1FWCbP6+/tb9i+Pg1DeFkp2oZ3q7jE1H0piVfWqMRBVVLjyPuEZRwpcjVuNp0pebFfK4HmivLc4hkzdcy65Djkmtb9VBNw60UqrxqQSDLYre1VFOFVrhOclvbfUvq9zZSBHJlZycqvItXXOveL92k8ahmEYhmHsAPjHh2EYhmEYHUXXyi7jx4+PMWPGSFmC9BbptEz7kEIifaTkEhXgSd0KV/JBfkcFClNUGaHGWpXER9F0fJ+2oZeCkrpI55MirLoNr25p07aEuiVP+3Huqm6kK/mB/SM1rShylTCrKsEax8E2GOSHUpLyxFL0aR437aloZJWMUckrnHv241e/+lXpuQMOOKBlH0ml01YMiKfGxGBuU6ZMKcrZ00atF5Y5PpVQkvS+8iTKbSuZjfuV5d13371lG4qGp/3zfq7jraRkRxVsSwU2zKjqVxMcE+db2Yy2VYG68nOEkgNUkj8Gw+PeUR4/uV7l7aIS1qnzTnm4ZJlNJefjumXb3FdsW52JrTxX3qyPrdZUO86zZj4MwzAMw+go/OPDMAzDMIyOomtll6Ghoejp6ZFBruoGfsp1toLKv6AC6Ch6Pd8kVnkh+BypX/ad4ybVpbxPqjwvWC/7y5vSrEvdulYyVB63uh1Ne7I9lilDVUlardrKUEHYOD56CpBKzflqmqiSvdRN7zq3+hU4r3w307hKvlD7gVIQpUbaRgXLevLJJ0t1MWgY+8V3aAOuOxUgkGV6YtEzLfdXBRRU63/ixIlFWQVfqpLZ6uSd4ri515W0puarStrkPKnzi14iyh4cAz+vCnilPMHYD86fkheV91yWH1ivCuCmvOFY3mOPPVp+rtZBbkOdD0pqV3OvghlWSR9KVlVnJO3Bfa/WQT7HVB6iVjKnc7sYhmEYhtG18I8PwzAMwzA6iq6VXV577bXYsmWLpIkIRQPmfysKVdHLStohham8DHJdisZUNKTKM6Kez8F4lKyhPBtyYJlW7VUF2FJQN/lJ9bNtpswmxU47MxV9lsaU1wHBcahAcKxXeQdk2UR5KihvC0UJqxwS6sZ8HodKF06JiZSwStdNCWzp0qVFee+99y61vf/++xflu+++uyhzTPT0ULlIBgcHizJtsH79+pbjiYiYOnVqUaadKe3UCapFqPWRzx8lPVbJBk0oabNOTqEMtaeVZwihAvlVBcBTkoNCu4HIqqRsPvfYY48VZUpoXM9cd9yTKldU3aByfF99r3DOVCA/JZHm4IecA+V1qL4nlKynxloVNFJJyE1YdjEMwzAMo2vhHx+GYRiGYXQUXSu7vP7667FlyxaZ1p50U5XHg/JwIW2mAqfwGVLQiv7OlJPyziFtpm4ZK1lIUXbPP/98qW3mZyGU3VQQmzrBdKpu4nNMpMLZP9Lt733ve1s+X4eujdBBlhT1qCQwgvQix1pFT5LCVvKKapuyEqUSdTs9Qq8X9unZZ58tysqLiXPP5x955JGW5YiIX/7yl0WZtKzqhwLlN0qbDDg2Y8aM0jsM2qa8m1TqcCVlKCq8Koig8ghQ3nqqr2q/qcBnEeVzimcL94LK76ECLKqAexHluVH5stReog24r9hvlrO0SfuovEBKouX8sZ468nj+m/ou4vcV+6SCeKm1kueb76jvEiVdKU8gJdVW5ctSfWzayrKLYRiGYRhdC//4MAzDMAyjo+ha2WXMmDEy/W9EdfAfgtSSuv1N2p80mLpVrEAK8s361ao90mOk70hl8ea/og4jykFtFOVHylpRdipfAL1r8rhV8CXSoaSz991336LMsZKGJ12ockLk9+ugTjpztYYytUyb11kvinrnvKg1WyVj0G5cR5w/BgqjbTkvXDecI/YvouzhwuBNzAGjAqpRzuR6Zr9nz55dlLO3C23IttV6UZ8Tag1Vebuo+VASJGU5QnlhqLVS9TeVT4dQ/ebnWWZWAeBUkDeCe4TPKLk7y+aqDe5jrn8lH6k5puyb5QcVEE+hztlCVM2x6gffUXtd5XlRUHMXUbZPK2QvnSqY+TAMwzAMo6Pwjw/DMAzDMDqKrpVdGo1GbNmyRXovqIBXme7mOyp9OqGCVClPA0ozuU6+X8fzhc/zhj+pacodVYGHSNerQFqKtlSBjlQejUxBKs8NtkGZiJ46rEvdCldBmTJUoDC1jlSgKI616ka6WiNsQ3kb1fGcIDJdrjyUFI2vvAvY3gsvvFCUaRsGFYsozwHtQ6pfeZAoLwXKErfffntRzrILA56pXEXcVyqgFPtH+prlvM6VVMC+Kw82Pq88J6rWNvuuUp4ryZRQ0jbfrZK6eM4o2ZhjYjAwgrZR51Juu05wL3VGKkmEEnyeb8qNtLOS8tS8UKbmeNRZmaECYSq5XH1HqfxH+Vxj3ymbtYK9XQzDMAzD6Fr4x4dhGIZhGB1F18ouo0ePjjFjxkjPCRVYrOpGukofXSeICkGKj5Rbll1IqfFvpM1IbSsPEpWHg/3O42bfGRRIyQ8qJTufpy15ozxTcaRpc86ZJlTaa7ZXx1so08kqyJKiX5UEQNRZBxnKw0LdVKfUwno5d1V9Uuuc9LfKYaRswzoPPvjgokwZJNelaHhSyuzTE088UZQHBgaKMnP8TJ8+vSjfdtttpbZpB2VD5UXD+VZ0fpUMooJLqeBxXKtq7dTNt6EocyUVq5xCnJc6eT/y31QeFq7bdnMb1Qn8F1GebyUn1MmLpXL5ZA8hBmGjDXgWqvXCtabOWu6dvCeVB5CS3+p8P6rzP69zJQHxnWb/LLsYhmEYhtG18I8PwzAMwzA6Cv/4MAzDMAyjo+jaOx+vvvpqDA0NSTdA5UJbBWpiyj2Qz7Btgm2zT9Q2c110A1TRHunmVQfsd5UmTKh7DQT7raKj8q5KbqvKPbGJOi6FSrslcgTQOmtBuVjWaVu54OZ/K81cabH8XN2bYDmPm+/zvhA1WGrWfF6txw984ANFmZEss3sm7/XQzZTPTZo0qShz3bINzh37xDtFs2bNKrXNPcP2lE5d5/4UbVZ1B4n/VnfTiDpRUGkb3iWoSiTIexvKbVfdW+I6YtsqYVn+t3J7Z71cO3SrV+eoaiuj6j5IE8rFuO79ljrgvqLd1L0eFc6hKkGqOhPqRO+tMz6VEDJC76VW7tF15qQJMx+GYRiGYXQU/vFhGIZhGEZH0bWyy4QJE2LMmDElCki58ajImRmsq04UPNJgddyYKK1EaBczRTeSGm03Yl+V26dyQ1NjUhIHaXHSdDlJlnLfU+U60R4JZZuIMlXNutqldbnWGHlTuWfWBdenikqqbKDo2ojyOChFULLgPmk3oR6TtpE6jyjbnO6yXCOkhFkvXW05l9yrlPhyYiy6ANO2ynWTNldyWp0oqBlVidiaqCMzKKk3P8+6VMJGJbHWabsqcZpyK2ZZnbXKjZl1VkVuVvNHcB1wXpSbL6FccHN/1XzTtip6r4p2rVyPI+olO+T8qfEpF3tVT4QOcdFK6nViOcMwDMMwuhb+8WEYhmEYRkfRtbLL+vXro6+vr9atXVJapGgj2qfxCVKHbEN5IGSQgqoj+dSJ6EmQQttzzz1Lf2N7KiGcovH5LpPa0TNBRReMKNtK3cCu0yeiTnTbiPJ8UFpQ4yNUNMqnn366KGePpjr9IGgbRoHkOqB3gEpMlu3EMVHuUMnFlJ1Zz7PPPluU3/ve9xblBx54oPQOk4UxISLf2X333Ysyo5f+6le/KsoqMZySIiLKY50xY0ZRvvvuu4uyopRVMj9F21clUCQ4ryp6JqESham9E1EeE2U2tQ4ItTZV1M7sQaa8ppTEoeysos1WeaKouog6NlfgGVclTaoopewv7UQb8kzlHlFydYSW0FSCVSVlK/lGRfjN79MmraReRzg1DMMwDKNr4R8fhmEYhmF0FF0ru+yyyy4xevToWonGqoJRqcBkdQJb1aHnSY3m/rHvDPyk6FrSkKS0SNmxTlLIua+q76T9SXOzT/RsUAHDqm5mqyBEipJTN8E5VkU7Vt2MV8mtFB2tAsZNmDChKFcFblLJmZRUoxJSKa+KKvmB2B6ZjXM/efLkonzNNdcU5exRRqmT9dLOlFco59SRQvfdd9+i/NRTT5WeU7Ls3Llzi/J1111XlBWlz/XFdcf+ZRpenSEq+BjXB9cEn2F7tHOVfECpgKgjc3K/qfWfvdkIFVyNtq0jVdZZ/7le9pfnax3Qnip5JiXEiHKiOSXfcRyUGtlvyqpcE1ybOSEn/6bWi5LNuL9V8LgqqUtdGSCaa/UtCzI2NDQU3/72t2PKlCkxbty4mDp1anz3u9/dSvebP39+DAwMxLhx4+K4446L1atXt9OMYRiGYRgjGG39+LjoooviBz/4QSxatCgeeOCBuPjii+NP//RP4/vf/37xzMUXXxyXXHJJLFq0KJYvXx79/f1x/PHHly7fGIZhGIbx7sWoRhuB7U866aSYOHFi/PVf/3Xx2R/8wR/E+PHj4+///u+j0WjEwMBAzJs3L84777yI+C2dOXHixLjooovijDPOeNM2BgcHY8KECXHyySfH6NGj5S1c0juKOowoU0iKolKoQ1uqtuqC1KrKo6ICZxFVHiBKzqlz612BEkym4UljKtpf3ZJXbSv7VwWVU6iidZtQUp6iPPM7hKKg1c1xJUWQ8mznVnkr0P511vMzzzxTlJcuXVp6jnOgPDfoEUNwHSnql2OlLBQR8fGPf7wo77bbbkWZuWQef/zxorx+/fo3bYPgHOU9VseTjlBSpQo21673W4T2duH4uJ5VwKsq1AmoRruxT3WC6VWdZQoquJdCnTMuy0WUcdlHJcvWkSDU99v22qCOV4s6d+t6E7b6fPPmzfFP//RPsWnTpq2ko4y2mI9jjjkmbrjhhlizZk1E/NadbenSpXHiiSdGRMTatWtjw4YNJb117Nixceyxx8ayZcta1vn666/H4OBg6T/DMAzDMEYu2rpwet5558WmTZviwAMPjN7e3hgeHo4LL7wwTjnllIiI2LBhQ0Rs/X85EydOjHXr1rWsc+HChfGd73xnW/puGIZhGMY7EG39+Lj66qvjxz/+cVx55ZUxY8aMWLVqVcybNy8GBgbi9NNPL55rJX0oCuqCCy6Ic845p/j34OBgTJ48OTZv3hyNRqMW/UQqKd/G5b+r5JkmVGAfFexJ3ZSO0BQcaVZSeaSpqrxJWtVZFQCJtGc7t5Ez2A/SrZmuVSm+FdRNfkUDV813nfbq0NlsW1HWuR6VB4TP8XY7514FH1MeMdlmddYL+8QxKQ8CeqWsXLmyKPf395fqbf5PR4T2+KG3Sx35jZ8fccQRRTl7nFDmUZ4K9FLgern33ntbfq7WUKay68h3hEpfr+jyqmBZiv7meiFU3o8qbx4F9pdMNdvmHT+VQ0cFBKzKZVXH5nW+M5SsXSUBq+B9HCv7W8fLh+NhPdtyTrf7DvcO11oOKlcHzTmuK91FtPnj4xvf+Eacf/758bnPfS4ifpvUad26dbFw4cI4/fTTi0Npw4YNpYibGzdulJrv2LFja7sQGoZhGIbxzkdbdz5eeeWVrX6J9vb2Fr8Cp0yZEv39/bF48eLi75s3b44lS5bEUUcdtQO6axiGYRjGOx1tMR+/93u/FxdeeGHsvffeMWPGjFi5cmVccskl8aUvfSkifkv7zJs3LxYsWBDTpk2LadOmxYIFC2L8+PFx6qmnttWxsWPHbpMnQ6bc+G8VIIV0Fak1JZsoKadK8lG5UCi1kOark9uCz2QJQNHw7cpYfJ40YtWtbkoIKsWymlvahu0pSrGKaqzjQVKnLpU/IdOyKiCekvuUt4vKI0TUpfy5Bil9ML+Eao+3+FlmkKQInaab67YOpa8kmCeeeKIo//7v/37pHebGUNIJ6WyWaQOOSclbWfpQY1L7kmNS67+OR1/+G1HnDFHSTl1vPbVGuCaVF0ydOts9r/L76txQuZvq5DyK0PIwZRuVS0mdnew3v3uq5oLz104K+1wv9wul8jx3ao1syzwRbf34+P73vx9/9Ed/FGeddVZs3LgxBgYG4owzzog//uM/Lp4599xz49VXX42zzjorXnzxxTjyyCPj+uuvbzsCnWEYhmEYIxNt/fjYdddd49JLL41LL71UPjNq1KiYP39+zJ8/fzu7ZhiGYRjGSETX5nZ5Myg6bVtu6irqSwU9Ut4BuW1KLQcccEBRJkXFZ1Rab1UnKbf8PNugtKPe4eekBdUN77pQ1DSpQyUh8HPlZbItgd0UlLcKoW7uR2hpR3lQtSu11IUKuETPBHqsKE8srn9KFM8//3ypPUVHq4BGhNp7XGvMXTJ16tTS+9xzlBa4nlkX5RXuJco39Daq8gZR0mMdiUnl6qhLo9fZlyrHTB1vr6pzrY6sWud8VhJHXfmhDuqcMwp5Xav5ruNFSbmP57GSrbJd1bnNdcS9rq4JKA8X9XlExKZNm4oy9wnba77TzneEs9oahmEYhtFR+MeHYRiGYRgdRdfLLopCJiXWTmCTVlDpkVWZ/WA5p2onPclU4KTX2LYKnlUn50v2iCG9qULWq1TLdTw1qlAnxwChgg0p+lrJXhHtU6sqD02dPEBZ6qojqRB1ctrUqT9C59DhvDIY2JQpU4oyqV9FeVP6yDF7HnnkkZb9ZayfHASvFbjOmaflE5/4RFHO800vAr5P6YTrnLf6OW5SyyoAW6aj1fnQLkipM319VbA+RaurdcTnWS8dASgN8HP2KaJsW4Jt8506e2F7ofYG9yj3CNe5Wpv5LFG25ZjYD7bNdcr2XnjhhaLM9ZjnW/WR9bIfKmgY9wjPBvY1f1+wL9wn/G5o2rMdmczMh2EYhmEYHYV/fBiGYRiG0VF0rezSKq+LChJDiqkqVLuiSVVwI4Jt8LZy1c1xFfSIZVKYStpR75IWzBRhnaBoHCupafW88izI8ogKlEQoOYeUN59hnUp+y33hmBQVq3KR0LuD46YcU5VPp05ab5XDgjZQz2SbK48TRXkz3wkpdpW3iDR1puH32muvovzcc88VZaayV/uNczFjxoyW5SrvB7VGOK+//vWvWz7DNaw8GdhelSedmr86ActoA0WLZ2lZnRvsB8fKtUp7qpwqlGDymcp3OA4lLSgZi1D7pSp3U52cOISyJ+3Pz7PN1fypM1Wdo7ST8jLJ46bUqfYP5TR1Zim5qZXnShN1zvDm+3Vk9ibMfBiGYRiG0VH4x4dhGIZhGB2Ff3wYhmEYhtFRdO2dj82bN2+lDyoXyyqdTmmM1L7quGeq6HPb4mJHPY86vIouqcatkvxU9Ut9Th2XejL7xLsBKspeRp3oi9QeVaRPlSBt+vTppXqffPLJoqzc/RSU5q00dWq3Efr+gZo/lmkDlXCOyNpqHa31kEMOafm5ctNVGnS+A0A7UJueNm1aUaYbodK5OVZq6rRN3t9qz/A55QrJ59le3Wigqr88H1T/VAJL2kbd/cnPUd/nXCi3fGVPPlN1B0DdZWO9yjYEP1f2yO8q93Te71IJz9R9ubou71V37Fq9r85qdQ+LLq7ZJZmReetEQ2bb+ZxqgvNdFR5BnQ9Ec01UfRds9U7tJw3DMAzDMHYA/OPDMAzDMIyOomtllyeffDJ6e3vj0UcfLT5TkUhJU+eEPCriHGk60vNKwlGRN5XbbP4b66ojqdA9kBEen3766aL8wAMPFGW6/+b2COWKTJqV7yopocrtk7ZVlF27spKab66PiPJcsh/K3YzuaYzed/TRRxdlRupkYqUc4VQlC7vjjjuK8gc/+MGivMcee0QrKPfme+65pyjvu+++pXcYgVS5AXKOb7/99qJMG5x22mkt+81nMtWrIujSzqR4VfRRFZm1KpIo6z322GOL8s9//vOW/aX7L9cKJQOV1DFLAJxvnjtcn0pGUVKjkgOq5B+2p+pVCeD4rnIRzlQ622DCTNqT607JNO26wkeUI0XzOSXvHn744UX5pptuKsp1kh5WJZZT0aXrhFdge3yXn+fkjQSjotZZg6yXY1CSYD7PVdJRvt+ci3YiTJv5MAzDMAyjo/CPD8MwDMMwOopRjXYywXQAg4ODMWHChDjyyCOjr69PUoEE6bFMT5JqJiXEdx566KGinKM3vhlIIR900EGlv5EWXLFiRVHmzX/S2aTkOW72af369UWZtH2eRo6PEg7pPBXdkEm9SJ0zeiWp9kzLsr+UAygh0DZKPlK24VjplRJRHittSHuQ7iVtzLr6+/uLMm+Ls/6TTjqp1Dbbu+6664oyZRs+w7o4Pnrw/P3f/31R3m+//YpypqMpGdE+M2fOLMrLli0rypMmTSrKSp569tlni/LAwEBRXrlyZalt9veII44oylzz7BNtwLq4JlRE1Oyxw7XHPtIejz32WFFm5FqV0JBgX7MHgoqyrDx4eP6wXiXNcM3mdU7vB9qA+4dRbNn2gw8+WJSVTKk8ISIidt9996LMNbzPPvsUZZ6Lyh60uVprGQ8//HDLz7lGaEOuR9pMSVoq0m1EeZ64png+sC7OGeVa9ZWr1mCG8oJSsgvnlec5z2Mlv2VwHU2dOnWrtoeGhuLOO++MTZs2bXUVIMPMh2EYhmEYHYV/fBiGYRiG0VF0rbfL6NGjo6+vT97SVkl+8vMq2I3ytiCtRApO9UN5xESUaWcV1IyU3f3331+UmViLz6jb0TmQDANxkW5UFCPtQXqRt8vVTfx8w5mUKyk8JiCj5wb7Xsc2Bx98cFGm90KEvoVeJ9kXaXVSy6TwKZndddddpbYZVIttq6R9rOuJJ54oyqQr+Qxp0iwv8jkG+poyZUpRXr58eVFW9L6qk3Oa6dRnnnmmKNPLhJ5ZlD9VwjqV5I824zqIiJg1a1ZR5piUpKiobban9nT2qFAeFgrK00ydURs2bGjZp/wO9wypfsqWrJdjUmPlXGTvHxWQjRIr17xKCqlkNq4J7r2I8prkOqRMSnmYNud5QsmAe08lCIzQQRYpXfGsvfTSS4vyf//v/70of+YznynKlGP+4R/+oShnyYd9+fSnP12U/+Zv/qYoU1JU3i4f//jHizLldc5j3iM8U6+44oqiTFmvKee3E3TTzIdhGIZhGB2Ff3wYhmEYhtFRdK3ssvPOO8fo0aMldUhaT90WjyhTfqQSSe2pYFaKqmQb+++/f1Fet27dVmNo1QapUVJ2fJ5UNuUK0t+k2TIFTEqNVCcDBKnAN7QNA9rUCVqU66VXDD9X+Qnq2IZSUPYwuvfee4syaVLOHz+n3TgmUorsE2lZ2j+iTBFz3amAdqSESSE/8sgjRblOoLrcHvu7Zs2aokybc9y0DfvB8WzcuLFlOaIsN9Fr56Mf/WhRJt37V3/1V0WZ3guUldgG7ZQ9Tri+6G2h5oJzTBpeBcBTHgT5ffadUPQ3P+cZpyj8Kk8bguNgnyjj8nMVlIxrljJehA6kRXso7yvS+xwfzyjKN3mcPG+5Lu6+++6irPL68NzmGaICZ1HiiSifQVxT/Jzrke//wR/8QVE+88wzW35O0B4R5b30j//4j0WZZxDtwTPgF7/4RVG+6KKLivKXvvSlosxAlpROIyI++9nPFmXaipJRcx05yJhhGIZhGF0L//gwDMMwDKOj6FrZZZdddonRo0eX6EnlbcFn8o1y0ooqhTYpaD5PKHpx9erVRZl0YUSZoqqTYl0FUSMVq6jiqv6qelW6adKy6ka/kmyq2iC2xzasM9PRe++9d1Emnc13SPcyyBjnnt4uDMrENZTzyhAck8qvQmpbQeWvyPSmyhvCsXLtcP1zfamU2NxjpFsjyuuec0Y5hp/ztj+D5lEyo/RUFQSQEiHHxD7S5kqqVDlmuB6zbeqko2ddyuOHMiDbUPu2qo+ESi0/efLkosxgWaqe7HFCG7If9CDhPqRUzHVLSYXByvh59qyiDMyznvuY64Vrk/anrMq1wvopzUSU8ykpeYES1bnnnluU1X6j3EE5JXugMUAgzzzmrmGwRo6P3jE8G/74j/+4KDNgIj1o8t++//3vF2XOTfM7oypAWYaZD8MwDMMwOgr/+DAMwzAMo6PoWtmlt7c3+vr6ZEAo0jtVdDTp7zpphlXgG1LWij7Nt8LpscKbz/RE4Th4Q5leA6TZFN2ag07VCR6k6lJjooyRg5oRHBPHSrr3tttuK8qkcevYpirHCb0kaDfah3PBQHCkbvku7cHcOKQ2I8oBvZYuXVqUOZe0ISlejoM3+ik/sP68zjlu5vxh8KVbb721KP/u7/5uy7qqgg2ptjl/9KrgPiH1TomDnzMoGeeCwbYoI0WUx0oJRgXj41nBZ7jvOUeqnggtHbIN5UWm8m0oLxPaKULvP7X3OGcqCJfy9MvjXrt2bVGmTES73XDDDS3fZxvsE9fvf/kv/6UoU5bLoN3oEci9xPFxv7I9rjvOKXMyRZTXOdeLOofZP34X0P7c65TusnxB+Yh5cBh0T53nKvfMX/zFXxRlnok8lyJ08ESekc310k6qODMfhmEYhmF0FP7xYRiGYRhGR9G1ssvYsWNjzJgxMhW3isGfbwmr2/uE8rYgGDyGt6NJ92XvBVLQvMlPuoq3j/k8oaQglaclQnuy0Fb8XNGvpG7ZBm+nZ68b3uYm7U8anqmumXOkjm0UxR1Rnj++r4JqkW5UwZ5YJ+lP3u6PKN/wJx3KdcG6SKszQBblFVKgHE+WRNSt/jlz5hRlpq8n5a2Cv/GZuXPnFmV6E0TovDJ/+Zd/WZS//e1vt3yfY1UB6ehlkL3RaFvOMSls0t/KQ4zvcn2pvDcR5bNFBd5SeVS43zgmPkOaP9PwlCHZHtct50LllWG9tCWfp0wQETF9+vSiTAmANle5s2hPPq+Cv2Vp85BDDinKt99+e1F+8sknizLXkZK0aD9KO5RalOdj7m+ds5bec5Rkf/aznxVlzl1edzwHlPcQvU9of3VuLFmypGWd2ZvtX/7lX6IVuE+a81QVfDLDzIdhGIZhGB2Ff3wYhmEYhtFRdK3s0tPTEz09PSXKSNFSVXS0CnpCuopUnroZz7KiWEkjRpSDdf3O7/xOy7ZZL8dErwrS2qRASY/lcZI2pTSkAkLxfUWTDgwMFGV6rlQFvGI/KAewfOihh7bsk0JVIKbDDjus5d/UXBJMra3mW6Vej9D5LJRng/KgYj20ZU5tTrBfpENvueWWokyJg/WyzP5RgrnnnnuKcqbwKTfxZvyJJ55YlElzU7JjvxcsWFCUr7322pZ9+tCHPlRq+5e//GVRVuuLYN85f8pzRclT+TnaUMmCKreUqpNl7r38N+W5xzLPSHoIcU3x+aogZly3v/rVr4oyzztlQzUvbIPrP3s3XXfddUX58ccfL8qnnnpqUaasx7W2atWqlmPgWUuvFEpKEeUzT9lHBYajrPH7v//7RZmy0o9+9KOinOUL/ptBv/7X//pfRVmtKYL2/O53v1uUL7zwwqL8kY98pPQO5b9FixYVZa6pZtt1rjk0YebDMAzDMIyOwj8+DMMwDMPoKLpWdtlzzz1jp512KgWcIcVHikkFzorQ9LnyflAUtEoLTdo4U2WKgiI1p8bBm8u8KX3JJZcUZdKLOeiQumWvnqGnB/tEmpRtVKUaV7KS6gfnTOVzyUHUWj0fUbY5b8BTDiAFSuqQsgTTSvMZBrwiXZtBu3EuGdyIa4eBjphLRgXAy23XCd7EPtETQqWG5/Ocb+YiiShLAryVz2BUpLDvu+++onzQQQcVZY6Pa562ufHGG0ttK+mK/SWNT6lSSQ6Um/juf/gP/6HUNs8ESsKcC65H9lXJXtxLVfmZOD7KvSr/FfcP179qQwXqys8xbwjn+JRTTolWUIEi2R49A7OkS+85rmGuNe43er5QHqQcw/woynskonxGKq9IdQWAdj7zzDOL8n/8j/+xKNOu3IcREZ/85CeL8v/+3/+7KNM7R3lfUfbl+LgXOI/sX0TEZz/72aLMPcO93pRVndvFMAzDMIyuhX98GIZhGIbRUfjHh2EYhmEYHcWoRjuZYDqAwcHBmDBhQpx44okxevRo6ZpFDa3K1Vbdz6iKitrqXUJFHK2686HcO+skKKL+R5dHapV53OqOCvuoXALVHQCV+Cjfx1BjVW7FddyYVSLBfOdDJcijeyG1adWeqqfKrU7pnepOEUF7qDVbtVXVODjf1Kw5l7zXwLsL/LzKjY/JsehSy/scM2fOLMrcr4zwyPHRvZwRcLMOr+7s8G7Ov//3/74oX3XVVS3rYj3U0bnHeFclomxbFRmW88e7FtxjHLdy9c/3x9S6UG2rvaSeUfszg+9wfdFllWec2ksMIUB36ny3ie7wPHcYBZfzwjADXM8PP/xwUebdB54NfDeifGfk7rvvLspq7jlWfo+dfvrpLZ//6U9/WpSrzjXeY2GCyLvuuqsocy6uv/76ovx//s//aVmn+q6LKN/5+Zu/+ZuizISZzfkbGhqKW2+9NTZt2rTVXs0w82EYhmEYRkfhHx+GYRiGYXQUXetqOzQ0FKNGjSrRoaSJSGGSViXdl/9NF0EmNqPLKj+/+uqri/IJJ5xQlEnZkZb9wQ9+UGqbLkqkbCdNmlSU6dKZkyg1QQqU7mWKOo8oU6uko0lvnnzyyUX57/7u74oyx3THHXcUZdqJLmLZ5qS8OSaO46mnnirKlHbotkmq/7bbbivK3/zmN4vylVdeWWqbVCnnidRxHXdE0pakT0l/k/KMKNuB42NyPtLtjMZKypVrjTZnkrjLL7+81PZZZ51VlJkIiutCRXllmfuKoJ2yvMixqoRbXP9ct/vtt19RZuI7ughzHWR6mJQ++075aNmyZUWZ8hvrJfXOsd58881F+WMf+1ipbc431zzdhDnfjOTLCK4f//jHi7I6Wy677LJS23RlVWfLo48+WpRJgbMfrJfrlHbOkXVVxF+eOZTNlOSj3JApUWQ5ma62nGMVnZiSgXI5VZGDeQZElN1zCTUmyrKsi9IOQwBUuTfzXKRNKMOr6Mlf//rXizJlUZ4njHydI7tSViU4puZYVTiFVjDzYRiGYRhGR+EfH4ZhGIZhdBRdK7vcdtttMWrUqJJkQDqUNCnpN9K4+d///M//XJRJBZJanT59elEmrXTnnXe2fJ43fkkVR0TcdNNNRfmoo44qypRnSAOTwmSUS4IUH+mx7HFCeYbP8f1169YVZd7ePuaYY4oyb56Tdqxr8z//8z8vymeffXZRnjp1alGmvEVpTCVkoxSRPUA4VuUppaJ1kqYmfcp6mISQNogoj5tlrgMmaaJMwLJaa1ybOaItJQDOMdck5Q7l6ZGjl7Z6JoNts1+MMEuam54vvKHPKJXcFyqJWoSeJ1LvXLeks0lBqzopH1TNN6npxYsXF2XS2ZQX2z1bsteHOluuuOKKokx5hXZjPzgvXIOcU3r/RGwthbRqg/Vy71Li4D6kbenNwSiaEeVzjeuIfeK88gyhzbn+mWyNe6QqySXbU2W+z+8rrnl+JynpL6I8/8pjju9wL9CGnPsVK1YUZcrxlA0jIm644YaizPFxjzVtbtnFMAzDMIyuhX98GIZhGIbRUXRtkLHdd989enp6SrQZqTnSw1U3l1WwKFJXpGJJGykKiXWqwFQZ6hY0P2cCLXVzmRQ026adIsoUKqlESjsMjqPGRLqdz9cN9MX5IBVYx258RgVdy1ABjdh39oOeGhwTqWZSnqwn30ivE7hOzZkKGsXPlYwUUV4jypuH9DKDMtFTgzID1ybrzPID6XO+Q7upQFr0hlLUO22ewb3E+SDFTtlFSTgqOFSd82B7wX5w33PvqfW0vf2iPViuShDGvnC+uS8pj6h3uReUTKY8UTJUcDaWubZpWyWv5HOJ65z7h/uq6v0m+N2lzt0c1I9rm2tVJeVUNqD3m5JpMjjHnA9KNc1nhoeH45FHHnGQMcMwDMMwug9dd+G0+Wus+WtN/aLlr/2qtO18X7EPdcqqzrrMhwrLXSe0smIZ1C/dunWpfqh6lc2rmA9lz3aZD6Iq3LNilfh/C2ocdcZatdbU+Ig6tq0zFxnKztuzf+rMY9321Drnu2qOqv7Pvs77aq3W2SOqvCPRbj8ytqdfdc+TOu1V7Y122qtTT0addVfnLFJ15nrVWqt6v9W7aq/neDbqLKtjT8XwVaWKINT7rfZ9q+9tha6TXZ588smYPHny290NwzAMwzC2AevXry8FvGuFrvvxsWXLlnj66aej0WjE3nvvHevXr39T7WgkYXBwMCZPnuxxv0vgcXvc7wZ43O+OcTcajXjppZdiYGBgq2SIGV0nu/T09MSkSZOKC2fvec973hWTluFxv7vgcb+74HG/u/BuGjcv9lbBF04NwzAMw+go/OPDMAzDMIyOomt/fIwdOzb+5E/+ZKtYCiMdHrfH/W6Ax+1xvxvwbh13HXTdhVPDMAzDMEY2upb5MAzDMAxjZMI/PgzDMAzD6Cj848MwDMMwjI7CPz4MwzAMw+go/OPDMAzDMIyOoit/fFx22WUxZcqU2GmnnWL27Nlxyy23vN1d2qFYuHBhHHHEEbHrrrvGHnvsEZ/61KfioYceKj3TaDRi/vz5MTAwEOPGjYvjjjsuVq9e/Tb1+K3BwoULY9SoUTFv3rzis5E67qeeeiq+8IUvxG677Rbjx4+Pww47LFasWFH8fSSOe2hoKL797W/HlClTYty4cTF16tT47ne/u1XSq3f6uG+++eb4vd/7vRgYGIhRo0bFT3/609Lf64zx9ddfj6985Sux++67x8477xyf/OQn48knn+zgKNpH1bjfeOONOO+88+Lggw+OnXfeOQYGBuKLX/xiPP3006U6Rtq4M84444wYNWpUXHrppaXP34nj3tHouh8fV199dcybNy++9a1vxcqVK+Pf/bt/FyeccEI88cQTb3fXdhiWLFkSZ599dtx2222xePHiGBoairlz58bLL79cPHPxxRfHJZdcEosWLYrly5dHf39/HH/88fHSSy+9jT3fcVi+fHlcccUVccghh5Q+H4njfvHFF+Poo4+O0aNHx89//vO4//7748/+7M/ive99b/HMSBz3RRddFD/4wQ9i0aJF8cADD8TFF18cf/qnfxrf//73i2dGwrhffvnlOPTQQ2PRokUt/15njPPmzYuf/OQncdVVV8XSpUvjN7/5TZx00km1s7q+Haga9yuvvBJ33XVX/NEf/VHcddddcc0118SaNWvik5/8ZOm5kTZu4qc//WncfvvtMTAwsNXf3onj3uFodBl+53d+p3HmmWeWPjvwwAMb559//tvUo7ceGzdubEREY8mSJY1Go9HYsmVLo7+/v/G9732veOa1115rTJgwofGDH/zg7ermDsNLL73UmDZtWmPx4sWNY489tvG1r32t0WiM3HGfd955jWOOOUb+faSO+xOf+ETjS1/6UumzT3/6040vfOELjUZjZI47Iho/+clPin/XGeOvf/3rxujRoxtXXXVV8cxTTz3V6OnpafziF7/oWN+3B3ncrXDHHXc0IqKxbt26RqMxssf95JNPNvbaa6/Gfffd19hnn30a//N//s/ibyNh3DsCXcV8bN68OVasWBFz584tfT537txYtmzZ29Srtx6bNm2KiIj3v//9ERGxdu3a2LBhQ8kOY8eOjWOPPXZE2OHss8+OT3ziE/Gxj32s9PlIHfe1114bc+bMiZNPPjn22GOPmDVrVvzwhz8s/j5Sx33MMcfEDTfcEGvWrImIiLvvvjuWLl0aJ554YkSM3HETdca4YsWKeOONN0rPDAwMxMyZM0eMHSJ+e86NGjWqYPxG6ri3bNkSp512WnzjG9+IGTNmbPX3kTrudtFVWW2fe+65GB4ejokTJ5Y+nzhxYmzYsOFt6tVbi0ajEeecc04cc8wxMXPmzIiIYqyt7LBu3bqO93FH4qqrrooVK1bEnXfeudXfRuq4H3vssbj88svjnHPOiW9+85txxx13xFe/+tUYO3ZsfPGLXxyx4z7vvPNi06ZNceCBB0Zvb28MDw/HhRdeGKecckpEjNz5JuqMccOGDTFmzJh43/vet9UzI+Xce+211+L888+PU089tcjuOlLHfdFFF0VfX1989atfbfn3kTrudtFVPz6aGDVqVOnfjUZjq89GCr785S/HPffcE0uXLt3qbyPNDuvXr4+vfe1rcf3118dOO+0knxtp496yZUvMmTMnFixYEBERs2bNitWrV8fll18eX/ziF4vnRtq4r7766vjxj38cV155ZcyYMSNWrVoV8+bNi4GBgTj99NOL50bauFthW8Y4UuzwxhtvxOc+97nYsmVLXHbZZW/6/Dt53CtWrIg///M/j7vuuqvtMbyTx70t6CrZZffdd4/e3t6tfv1t3Lhxq/9zGAn4yle+Etdee23ceOONMWnSpOLz/v7+iIgRZ4cVK1bExo0bY/bs2dHX1xd9fX2xZMmS+Iu/+Ivo6+srxjbSxr3nnnvGQQcdVPps+vTpxSXqkTrf3/jGN+L888+Pz33uc3HwwQfHaaedFl//+tdj4cKFETFyx03UGWN/f39s3rw5XnzxRfnMOxVvvPFGfOYzn4m1a9fG4sWLC9YjYmSO+5ZbbomNGzfG3nvvXZxx69atiz/8wz+MfffdNyJG5ri3BV3142PMmDExe/bsWLx4cenzxYsXx1FHHfU29WrHo9FoxJe//OW45ppr4pe//GVMmTKl9PcpU6ZEf39/yQ6bN2+OJUuWvKPt8NGPfjTuvffeWLVqVfHfnDlz4vOf/3ysWrUqpk6dOiLHffTRR2/lSr1mzZrYZ599ImLkzvcrr7wSPT3lI6a3t7dwtR2p4ybqjHH27NkxevTo0jPPPPNM3Hfffe9oOzR/eDz88MPxb//2b7HbbruV/j4Sx33aaafFPffcUzrjBgYG4hvf+Eb867/+a0SMzHFvE96mi64SV111VWP06NGNv/7rv27cf//9jXnz5jV23nnnxuOPP/52d22H4b/9t//WmDBhQuOmm25qPPPMM8V/r7zySvHM9773vcaECRMa11xzTePee+9tnHLKKY0999yzMTg4+Db2fMeD3i6Nxsgc9x133NHo6+trXHjhhY2HH3648Q//8A+N8ePHN3784x8Xz4zEcZ9++umNvfbaq3Hdddc11q5d27jmmmsau+++e+Pcc88tnhkJ437ppZcaK1eubKxcubIREY1LLrmksXLlysKro84YzzzzzMakSZMa//Zv/9a46667Gh/5yEcahx56aGNoaOjtGtabomrcb7zxRuOTn/xkY9KkSY1Vq1aVzrnXX3+9qGOkjbsVsrdLo/HOHPeORtf9+Gg0Go2//Mu/bOyzzz6NMWPGNA4//PDCBXWkICJa/vejH/2oeGbLli2NP/mTP2n09/c3xo4d2/jwhz/cuPfee9++Tr9FyD8+Ruq4/+///b+NmTNnNsaOHds48MADG1dccUXp7yNx3IODg42vfe1rjb333rux0047NaZOndr41re+VfryGQnjvvHGG1vu59NPP73RaNQb46uvvtr48pe/3Hj/+9/fGDduXOOkk05qPPHEE2/DaOqjatxr166V59yNN95Y1DHSxt0KrX58vBPHvaMxqtFoNDrBsBiGYRiGYUR02Z0PwzAMwzBGPvzjwzAMwzCMjsI/PgzDMAzD6Cj848MwDMMwjI7CPz4MwzAMw+go/OPDMAzDMIyOwj8+DMMwDMPoKPzjwzAMwzCMjsI/PgzDMAzD6Cj848MwDMMwjI7CPz4MwzAMw+go/h9Cy/vzI9PlygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cv2.cvtColor(state,cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30af2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9f20f",
   "metadata": {},
   "source": [
    "## Step 4- Setup callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a5520",
   "metadata": {},
   "source": [
    "installing PyTorch allows you to use all of the available callbacks, which are powerful tools for monitoring the training of a reinforcement learning agent. Some of the available callbacks in Stable Baselines 3 include Tensorboard, Checkpoint, and CallbackList. By using these callbacks, you can visualize and save the training progress of your reinforcement learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac9ab747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.15.2+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.0.2+cu118)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708fd003",
   "metadata": {},
   "source": [
    "With supervised learning, we can easily implement the cost function, run gradient descent on it, and be very confident that we’ll get excellent results with relatively little hyperparameter tuning. The route to success in reinforcement learning isn’t as obvious—the algorithms have many moving parts that are hard to debug, and they require substantial effort in tuning in order to get good results. PPO strikes a balance between ease of implementation, sample complexity, and ease of tuning, trying to compute an update at each step that minimizes the cost function while ensuring the deviation from the previous policy is relatively small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d0a334c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stable-baselines3) (1.21.5)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stable-baselines3) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata~=4.13 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stable-baselines3) (4.13.0)\n",
      "Requirement already satisfied: torch>=1.11 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stable-baselines3) (2.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stable-baselines3) (3.5.2)\n",
      "Requirement already satisfied: gym==0.21 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stable-baselines3) (0.21.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stable-baselines3) (1.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from importlib-metadata~=4.13->stable-baselines3) (3.8.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3) (2.11.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3) (4.3.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3) (1.10.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11->stable-baselines3) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11->stable-baselines3) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06d356ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "075db06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\n",
      "----------------------------- --------------------\n",
      "absl-py                       1.4.0\n",
      "alabaster                     0.7.12\n",
      "ale-py                        0.7.4\n",
      "anaconda-client               1.11.0\n",
      "anaconda-navigator            2.4.0\n",
      "anaconda-project              0.11.1\n",
      "anyio                         3.5.0\n",
      "appdirs                       1.4.4\n",
      "argon2-cffi                   21.3.0\n",
      "argon2-cffi-bindings          21.2.0\n",
      "arrow                         1.2.2\n",
      "astroid                       2.11.7\n",
      "astropy                       5.1\n",
      "astunparse                    1.6.3\n",
      "atomicwrites                  1.4.0\n",
      "attrs                         21.4.0\n",
      "Automat                       20.2.0\n",
      "autopep8                      1.6.0\n",
      "AutoROM                       0.6.1\n",
      "AutoROM.accept-rom-license    0.6.1\n",
      "Babel                         2.9.1\n",
      "backcall                      0.2.0\n",
      "backports.functools-lru-cache 1.6.4\n",
      "backports.tempfile            1.0\n",
      "backports.weakref             1.0.post1\n",
      "bcrypt                        3.2.0\n",
      "beautifulsoup4                4.11.1\n",
      "binaryornot                   0.4.4\n",
      "bitarray                      2.5.1\n",
      "bkcharts                      0.2\n",
      "black                         22.6.0\n",
      "bleach                        4.1.0\n",
      "bokeh                         2.4.3\n",
      "boto3                         1.24.28\n",
      "botocore                      1.27.28\n",
      "Bottleneck                    1.3.5\n",
      "brotlipy                      0.7.0\n",
      "cachetools                    5.3.0\n",
      "certifi                       2022.9.14\n",
      "cffi                          1.15.1\n",
      "chardet                       4.0.0\n",
      "charset-normalizer            2.0.4\n",
      "click                         8.0.4\n",
      "cloudpickle                   2.0.0\n",
      "clyent                        1.2.2\n",
      "colorama                      0.4.5\n",
      "colorcet                      3.0.0\n",
      "comtypes                      1.1.10\n",
      "conda                         23.1.0\n",
      "conda-build                   3.22.0\n",
      "conda-content-trust           0.1.3\n",
      "conda-pack                    0.6.0\n",
      "conda-package-handling        1.9.0\n",
      "conda-repo-cli                1.0.20\n",
      "conda-token                   0.4.0\n",
      "conda-verify                  3.4.2\n",
      "constantly                    15.1.0\n",
      "cookiecutter                  1.7.3\n",
      "cryptography                  37.0.1\n",
      "cssselect                     1.1.0\n",
      "cycler                        0.11.0\n",
      "Cython                        0.29.32\n",
      "cytoolz                       0.11.0\n",
      "daal4py                       2021.6.0\n",
      "dask                          2022.7.0\n",
      "datashader                    0.14.1\n",
      "datashape                     0.5.4\n",
      "debugpy                       1.5.1\n",
      "decorator                     5.1.1\n",
      "defusedxml                    0.7.1\n",
      "diff-match-patch              20200713\n",
      "dill                          0.3.4\n",
      "distributed                   2022.7.0\n",
      "docutils                      0.18.1\n",
      "entrypoints                   0.4\n",
      "et-xmlfile                    1.1.0\n",
      "fastjsonschema                2.16.2\n",
      "filelock                      3.6.0\n",
      "flake8                        4.0.1\n",
      "Flask                         1.1.2\n",
      "flatbuffers                   23.3.3\n",
      "fonttools                     4.25.0\n",
      "fsspec                        2022.7.1\n",
      "future                        0.18.2\n",
      "gast                          0.4.0\n",
      "gensim                        4.1.2\n",
      "glob2                         0.7\n",
      "google-auth                   2.17.1\n",
      "google-auth-oauthlib          0.4.6\n",
      "google-pasta                  0.2.0\n",
      "greenlet                      1.1.1\n",
      "grpcio                        1.53.0\n",
      "gym                           0.21.0\n",
      "gym-notices                   0.0.8\n",
      "h5py                          3.7.0\n",
      "HeapDict                      1.0.1\n",
      "holoviews                     1.15.0\n",
      "hvplot                        0.8.0\n",
      "hyperlink                     21.0.0\n",
      "idna                          3.3\n",
      "imagecodecs                   2021.8.26\n",
      "imageio                       2.19.3\n",
      "imagesize                     1.4.1\n",
      "importlib-metadata            4.13.0\n",
      "importlib-resources           5.12.0\n",
      "incremental                   21.3.0\n",
      "inflection                    0.5.1\n",
      "iniconfig                     1.1.1\n",
      "intake                        0.6.5\n",
      "intervaltree                  3.1.0\n",
      "ipykernel                     6.15.2\n",
      "ipython                       7.31.1\n",
      "ipython-genutils              0.2.0\n",
      "ipywidgets                    7.6.5\n",
      "isort                         5.9.3\n",
      "itemadapter                   0.3.0\n",
      "itemloaders                   1.0.4\n",
      "itsdangerous                  2.0.1\n",
      "jdcal                         1.4.1\n",
      "jedi                          0.18.1\n",
      "jellyfish                     0.9.0\n",
      "Jinja2                        2.11.3\n",
      "jinja2-time                   0.2.0\n",
      "jmespath                      0.10.0\n",
      "joblib                        1.1.0\n",
      "json5                         0.9.6\n",
      "jsonschema                    4.16.0\n",
      "jupyter                       1.0.0\n",
      "jupyter_client                7.3.4\n",
      "jupyter-console               6.4.3\n",
      "jupyter_core                  4.11.1\n",
      "jupyter-server                1.18.1\n",
      "jupyterlab                    3.4.4\n",
      "jupyterlab-pygments           0.1.2\n",
      "jupyterlab-server             2.10.3\n",
      "jupyterlab-widgets            1.0.0\n",
      "keras                         2.10.0\n",
      "Keras-Preprocessing           1.1.2\n",
      "keyring                       23.4.0\n",
      "kiwisolver                    1.4.2\n",
      "lazy-object-proxy             1.6.0\n",
      "libarchive-c                  2.9\n",
      "libclang                      16.0.0\n",
      "llvmlite                      0.38.0\n",
      "locket                        1.0.0\n",
      "lxml                          4.9.1\n",
      "lz4                           3.1.3\n",
      "Markdown                      3.3.4\n",
      "markdown-it-py                2.2.0\n",
      "MarkupSafe                    2.0.1\n",
      "matplotlib                    3.5.2\n",
      "matplotlib-inline             0.1.6\n",
      "mccabe                        0.6.1\n",
      "mdurl                         0.1.2\n",
      "menuinst                      1.4.19\n",
      "mistune                       0.8.4\n",
      "mkl-fft                       1.3.1\n",
      "mkl-random                    1.2.2\n",
      "mkl-service                   2.4.0\n",
      "mock                          4.0.3\n",
      "mpmath                        1.2.1\n",
      "msgpack                       1.0.3\n",
      "multipledispatch              0.6.0\n",
      "munkres                       1.1.4\n",
      "mypy-extensions               0.4.3\n",
      "navigator-updater             0.3.0\n",
      "nbclassic                     0.3.5\n",
      "nbclient                      0.5.13\n",
      "nbconvert                     6.4.4\n",
      "nbformat                      5.5.0\n",
      "nest-asyncio                  1.5.5\n",
      "networkx                      2.8.4\n",
      "nltk                          3.7\n",
      "nose                          1.3.7\n",
      "notebook                      6.4.12\n",
      "numba                         0.55.1\n",
      "numexpr                       2.8.3\n",
      "numpy                         1.21.5\n",
      "numpydoc                      1.4.0\n",
      "oauthlib                      3.2.2\n",
      "olefile                       0.46\n",
      "opencv-python                 4.7.0.72\n",
      "openpyxl                      3.0.10\n",
      "opt-einsum                    3.3.0\n",
      "packaging                     21.3\n",
      "pandas                        1.4.4\n",
      "pandocfilters                 1.5.0\n",
      "panel                         0.13.1\n",
      "param                         1.12.0\n",
      "paramiko                      2.8.1\n",
      "parsel                        1.6.0\n",
      "parso                         0.8.3\n",
      "partd                         1.2.0\n",
      "pathlib                       1.0.1\n",
      "pathspec                      0.9.0\n",
      "patsy                         0.5.2\n",
      "pep8                          1.7.1\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        9.2.0\n",
      "pip                           22.2.2\n",
      "pkginfo                       1.8.2\n",
      "platformdirs                  2.5.2\n",
      "plotly                        5.9.0\n",
      "pluggy                        1.0.0\n",
      "poyo                          0.5.0\n",
      "prometheus-client             0.14.1\n",
      "prompt-toolkit                3.0.20\n",
      "Protego                       0.1.16\n",
      "protobuf                      3.19.6\n",
      "psutil                        5.9.0\n",
      "ptyprocess                    0.7.0\n",
      "py                            1.11.0\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.8\n",
      "pycodestyle                   2.8.0\n",
      "pycosat                       0.6.3\n",
      "pycparser                     2.21\n",
      "pyct                          0.4.8\n",
      "pycurl                        7.45.1\n",
      "PyDispatcher                  2.0.5\n",
      "pydocstyle                    6.1.1\n",
      "pyerfa                        2.0.0\n",
      "pyflakes                      2.4.0\n",
      "Pygments                      2.15.1\n",
      "PyHamcrest                    2.0.2\n",
      "PyJWT                         2.4.0\n",
      "pylint                        2.14.5\n",
      "pyls-spyder                   0.4.0\n",
      "PyNaCl                        1.5.0\n",
      "pyodbc                        4.0.34\n",
      "pyOpenSSL                     22.0.0\n",
      "pyparsing                     3.0.9\n",
      "pyrsistent                    0.18.0\n",
      "PySocks                       1.7.1\n",
      "pytest                        7.1.2\n",
      "python-dateutil               2.8.2\n",
      "python-lsp-black              1.0.0\n",
      "python-lsp-jsonrpc            1.0.0\n",
      "python-lsp-server             1.3.3\n",
      "python-slugify                5.0.2\n",
      "python-snappy                 0.6.0\n",
      "pytz                          2022.1\n",
      "pyviz-comms                   2.0.2\n",
      "PyWavelets                    1.3.0\n",
      "pywin32                       302\n",
      "pywin32-ctypes                0.2.0\n",
      "pywinpty                      2.0.2\n",
      "PyYAML                        6.0\n",
      "pyzmq                         23.2.0\n",
      "QDarkStyle                    3.0.2\n",
      "qstylizer                     0.1.10\n",
      "QtAwesome                     1.0.3\n",
      "qtconsole                     5.2.2\n",
      "QtPy                          2.2.0\n",
      "queuelib                      1.5.0\n",
      "regex                         2022.7.9\n",
      "requests                      2.28.1\n",
      "requests-file                 1.5.1\n",
      "requests-oauthlib             1.3.1\n",
      "rich                          13.3.5\n",
      "rope                          0.22.0\n",
      "rsa                           4.9\n",
      "Rtree                         0.9.7\n",
      "ruamel.yaml                   0.17.21\n",
      "ruamel.yaml.clib              0.2.6\n",
      "ruamel-yaml-conda             0.15.100\n",
      "s3transfer                    0.6.0\n",
      "scikit-image                  0.19.2\n",
      "scikit-learn                  1.0.2\n",
      "scikit-learn-intelex          2021.20221004.171935\n",
      "scipy                         1.9.1\n",
      "Scrapy                        2.6.2\n",
      "seaborn                       0.11.2\n",
      "Send2Trash                    1.8.0\n",
      "service-identity              18.1.0\n",
      "setuptools                    63.4.1\n",
      "sip                           4.19.13\n",
      "six                           1.16.0\n",
      "smart-open                    5.2.1\n",
      "sniffio                       1.2.0\n",
      "snowballstemmer               2.2.0\n",
      "sortedcollections             2.1.0\n",
      "sortedcontainers              2.4.0\n",
      "soupsieve                     2.3.1\n",
      "Sphinx                        5.0.2\n",
      "sphinxcontrib-applehelp       1.0.2\n",
      "sphinxcontrib-devhelp         1.0.2\n",
      "sphinxcontrib-htmlhelp        2.0.0\n",
      "sphinxcontrib-jsmath          1.0.1\n",
      "sphinxcontrib-qthelp          1.0.3\n",
      "sphinxcontrib-serializinghtml 1.1.5\n",
      "spyder                        5.2.2\n",
      "spyder-kernels                2.2.1\n",
      "SQLAlchemy                    1.4.39\n",
      "stable-baselines3             1.8.0\n",
      "statsmodels                   0.13.2\n",
      "sympy                         1.10.1\n",
      "tables                        3.6.1\n",
      "tabulate                      0.8.10\n",
      "TBB                           0.2\n",
      "tblib                         1.7.0\n",
      "tenacity                      8.0.1\n",
      "tensorboard                   2.10.1\n",
      "tensorboard-data-server       0.6.1\n",
      "tensorboard-plugin-wit        1.8.1\n",
      "tensorflow                    2.10.1\n",
      "tensorflow-estimator          2.10.0\n",
      "tensorflow-hub                0.13.0\n",
      "tensorflow-io-gcs-filesystem  0.31.0\n",
      "tensorflow-text               2.10.0\n",
      "termcolor                     2.2.0\n",
      "terminado                     0.13.1\n",
      "testpath                      0.6.0\n",
      "text-unidecode                1.3\n",
      "textdistance                  4.2.1\n",
      "threadpoolctl                 2.2.0\n",
      "three-merge                   0.1.1\n",
      "tifffile                      2021.7.2\n",
      "tinycss                       0.4\n",
      "tldextract                    3.2.0\n",
      "toml                          0.10.2\n",
      "tomli                         2.0.1\n",
      "tomlkit                       0.11.1\n",
      "toolz                         0.11.2\n",
      "torch                         2.0.1\n",
      "torchaudio                    2.0.2+cu118\n",
      "torchvision                   0.15.2+cu118\n",
      "tornado                       6.1\n",
      "tqdm                          4.64.1\n",
      "traitlets                     5.1.1\n",
      "Twisted                       22.2.0\n",
      "twisted-iocpsupport           1.0.2\n",
      "typing_extensions             4.3.0\n",
      "ujson                         5.4.0\n",
      "Unidecode                     1.2.0\n",
      "urllib3                       1.26.11\n",
      "vizdoom                       1.1.14\n",
      "w3lib                         1.21.0\n",
      "watchdog                      2.1.6\n",
      "wcwidth                       0.2.5\n",
      "webencodings                  0.5.1\n",
      "websocket-client              0.58.0\n",
      "Werkzeug                      2.0.3\n",
      "wheel                         0.37.1\n",
      "widgetsnbextension            3.5.2\n",
      "win-inet-pton                 1.1.0\n",
      "win-unicode-console           0.5\n",
      "wincertstore                  0.2\n",
      "wrapt                         1.14.1\n",
      "xarray                        0.20.1\n",
      "xlrd                          2.0.1\n",
      "XlsxWriter                    3.0.3\n",
      "xlwings                       0.27.15\n",
      "yapf                          0.31.0\n",
      "zict                          2.1.0\n",
      "zipp                          3.8.0\n",
      "zope.interface                5.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\lenovo\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28d2560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import calbacks class from sb3 \n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4fc9db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d868d",
   "metadata": {},
   "source": [
    "This is a custom callback class TrainAndLoggingCallback that extends the BaseCallback class from the stable_baselines3 library. Callbacks are a way to customize the behavior of the reinforcement learning algorithm.\n",
    "\n",
    "In this case, TrainAndLoggingCallback is used to save the model every check_freq steps during training, and to log training progress. The save_path parameter specifies the directory where the model checkpoints will be saved. If save_path is not None, the callback creates the directory if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "547be512",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_basic'\n",
    "LOG_DIR = './logs/log_basic'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0146e63a",
   "metadata": {},
   "source": [
    "These two variables are paths to the directories where the trained model checkpoints and logs will be saved, respectively.\n",
    "\n",
    "CHECKPOINT_DIR points to the directory where the model checkpoints will be saved during training. A checkpoint is a snapshot of the model's current state that can be used to continue training later, or to load and evaluate the trained model at a later time.\n",
    "\n",
    "LOG_DIR points to the directory where the logs generated during training will be saved. The logs include various metrics such as the training and evaluation episode rewards, number of steps taken, etc. that can be used to monitor the training progress and evaluate the performance of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b478dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0310d850",
   "metadata": {},
   "source": [
    "The TrainAndLoggingCallback is a custom callback that we have defined earlier to save the model at regular intervals during training and to log the training statistics using TensorBoard. The check_freq argument specifies how often we want to save the model (in terms of number of training steps), and the save_path argument specifies the directory where we want to save the model checkpoints.\n",
    "\n",
    "We have set check_freq to 10,000, which means that we will save the model every 10,000 training steps. We have also specified CHECKPOINT_DIR as the directory where we want to save the checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b55df",
   "metadata": {},
   "source": [
    "## Step 5- Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "003f2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ppo for training\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da8861",
   "metadata": {},
   "source": [
    "We imported PPO algorithm from stable_baselines3, which we will use to train the agent on the Doom environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd8f704",
   "metadata": {},
   "source": [
    "PPO (Proximal Policy Optimization) is a popular reinforcement learning algorithm that has shown good performance on a variety of tasks. It is known for its stability and sample efficiency, which makes it a good choice for training agents in complex environments. It is an on-policy algorithm, which means that it learns the optimal policy by repeatedly sampling the current policy and improving it in an iterative fashion. PPO also uses a clipping mechanism to ensure that the policy update stays within a certain range, which helps to prevent the policy from changing too much at once and destabilizing the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeef2f20",
   "metadata": {},
   "source": [
    "Simple Q-learning is a model-free, off-policy, temporal-difference algorithm that learns to estimate the optimal action-value function by using a table to store the value of each state-action pair. However, Q-learning can be limited in its ability to handle high-dimensional or continuous state spaces, and it can be computationally expensive to store and update a table of values for each state-action pair.\n",
    "\n",
    "On the other hand, PPO (Proximal Policy Optimization) is a model-free, on-policy, actor-critic algorithm that can handle high-dimensional and continuous state spaces through the use of neural networks. PPO is also designed to be more sample-efficient than other policy gradient methods and can learn directly from raw sensory input, making it a good choice for training agents in complex environments like video games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c1ed30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment\n",
    "env = VizDoomGym()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50f2d2",
   "metadata": {},
   "source": [
    "creating a non-rendered environment without visualization can be useful for training RL agents when you don't need to see the agent's actions in the environment.\n",
    "\n",
    "Also, the non-rendered environment can be faster because it does not need to process and display images on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4c7898e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.0001, n_steps=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47504fc0",
   "metadata": {},
   "source": [
    "This code initializes a new PPO agent with a convolutional neural network policy (CnnPolicy), using the VizDoomGym environment we defined earlier as the training environment.\n",
    "\n",
    "The tensorboard_log argument specifies the directory where TensorBoard logs will be saved for visualization purposes.\n",
    "\n",
    "The verbose argument sets the level of output printed to the console during training. A value of 1 means that progress will be printed every 10,000 steps.\n",
    "\n",
    "The learning_rate argument sets the learning rate used by the optimizer during training. A value of 0.0001 is a commonly used value.\n",
    "\n",
    "The n_steps argument determines the number of steps to take between updates to the policy. A value of 2048 is a commonly used value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ad3bcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/log_basic\\PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.8     |\n",
      "|    ep_rew_mean     | -41.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 58       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 28.7         |\n",
      "|    ep_rew_mean          | -60.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 36           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 112          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024942823 |\n",
      "|    clip_fraction        | 0.0998       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -5.85e-05    |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 984          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 0.00108      |\n",
      "|    value_loss           | 2.87e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | -71.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 32          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 190         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010570605 |\n",
      "|    clip_fraction        | 0.0924      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0938      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.43e+03    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.000302    |\n",
      "|    value_loss           | 3.35e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 24.2        |\n",
      "|    ep_rew_mean          | -27.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 270         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012926817 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.62e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00262    |\n",
      "|    value_loss           | 3.23e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 24.3       |\n",
      "|    ep_rew_mean          | -36.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 29         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 348        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02533238 |\n",
      "|    clip_fraction        | 0.404      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | 0.452      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.81e+03   |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | 0.00812    |\n",
      "|    value_loss           | 3.74e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20.4        |\n",
      "|    ep_rew_mean          | -11.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 429         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012252508 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.56e+03    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.0057      |\n",
      "|    value_loss           | 3.42e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 25.5        |\n",
      "|    ep_rew_mean          | -47.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 508         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020240001 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.979      |\n",
      "|    explained_variance   | 0.513       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.58e+03    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.0218      |\n",
      "|    value_loss           | 3.64e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 26.7        |\n",
      "|    ep_rew_mean          | -59.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 589         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023449317 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.975      |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.68e+03    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.00785     |\n",
      "|    value_loss           | 3.16e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 17.1         |\n",
      "|    ep_rew_mean          | 2.95         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 669          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0135088535 |\n",
      "|    clip_fraction        | 0.194        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.878       |\n",
      "|    explained_variance   | 0.549        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.71e+03     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | 0.0036       |\n",
      "|    value_loss           | 3.48e+03     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 19.4       |\n",
      "|    ep_rew_mean          | -6.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 27         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 750        |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01422472 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.888     |\n",
      "|    explained_variance   | 0.683      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.81e+03   |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | 0.0101     |\n",
      "|    value_loss           | 2.92e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 13.4       |\n",
      "|    ep_rew_mean          | 33.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 26         |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 834        |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02714693 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.936     |\n",
      "|    explained_variance   | 0.616      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 2.78e+03   |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | 0.00696    |\n",
      "|    value_loss           | 3.89e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20.4        |\n",
      "|    ep_rew_mean          | -11.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 922         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042272516 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.891      |\n",
      "|    explained_variance   | 0.486       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.14e+03    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.0193      |\n",
      "|    value_loss           | 4.97e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 14.6        |\n",
      "|    ep_rew_mean          | 25          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 1009        |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065365866 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.849      |\n",
      "|    explained_variance   | 0.424       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.52e+03    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | 0.0411      |\n",
      "|    value_loss           | 3.65e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 18.4        |\n",
      "|    ep_rew_mean          | 1.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 1091        |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022251643 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.763      |\n",
      "|    explained_variance   | 0.686       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.13e+03    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | 0.00622     |\n",
      "|    value_loss           | 2.76e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 12.6        |\n",
      "|    ep_rew_mean          | 37.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 1183        |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037979357 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.739      |\n",
      "|    explained_variance   | 0.651       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.12e+03    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | 0.0111      |\n",
      "|    value_loss           | 2.65e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15.4       |\n",
      "|    ep_rew_mean          | 20.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 1274       |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03977982 |\n",
      "|    clip_fraction        | 0.237      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.723     |\n",
      "|    explained_variance   | 0.743      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 912        |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | 0.02       |\n",
      "|    value_loss           | 1.94e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.7        |\n",
      "|    ep_rew_mean          | 50.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 1360        |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016703704 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.602      |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 742         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.00334     |\n",
      "|    value_loss           | 2.06e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.16        |\n",
      "|    ep_rew_mean          | 61.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 1451        |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058019802 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | 0.591       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.03e+03    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 0.00485     |\n",
      "|    value_loss           | 3.31e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9.81       |\n",
      "|    ep_rew_mean          | 57.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 1538       |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05341007 |\n",
      "|    clip_fraction        | 0.333      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.724     |\n",
      "|    explained_variance   | 0.711      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 747        |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | 0.0572     |\n",
      "|    value_loss           | 1.74e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.98       |\n",
      "|    ep_rew_mean          | 72.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 1630       |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02606467 |\n",
      "|    clip_fraction        | 0.237      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.731     |\n",
      "|    explained_variance   | 0.71       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 852        |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.000127  |\n",
      "|    value_loss           | 2.31e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.52       |\n",
      "|    ep_rew_mean          | 70.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 1727       |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03464886 |\n",
      "|    clip_fraction        | 0.343      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.714     |\n",
      "|    explained_variance   | 0.648      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 503        |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | 0.0143     |\n",
      "|    value_loss           | 1.09e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.72       |\n",
      "|    ep_rew_mean          | 77.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 1822       |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08396512 |\n",
      "|    clip_fraction        | 0.411      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.67      |\n",
      "|    explained_variance   | 0.442      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 352        |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | 0.0367     |\n",
      "|    value_loss           | 692        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.09       |\n",
      "|    ep_rew_mean          | 76.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 1916       |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05782875 |\n",
      "|    clip_fraction        | 0.274      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.551     |\n",
      "|    explained_variance   | 0.465      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 165        |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | 0.0149     |\n",
      "|    value_loss           | 353        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.23        |\n",
      "|    ep_rew_mean          | 80.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 2016        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039357755 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.517      |\n",
      "|    explained_variance   | 0.557       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 57.4        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | 0.00425     |\n",
      "|    value_loss           | 236         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.43        |\n",
      "|    ep_rew_mean          | 84.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 2115        |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030840874 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.413      |\n",
      "|    explained_variance   | 0.591       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 159         |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00029    |\n",
      "|    value_loss           | 200         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.78        |\n",
      "|    ep_rew_mean          | 82.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 2210        |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029367408 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.316      |\n",
      "|    explained_variance   | 0.621       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 37          |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | 0.037       |\n",
      "|    value_loss           | 92.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.57        |\n",
      "|    ep_rew_mean          | 83.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 2314        |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042695113 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.303      |\n",
      "|    explained_variance   | 0.572       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 93.7        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | 0.0313      |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.78       |\n",
      "|    ep_rew_mean          | 82.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 2418       |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07372089 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.236     |\n",
      "|    explained_variance   | 0.447      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 139        |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | 0.0133     |\n",
      "|    value_loss           | 200        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 17.5      |\n",
      "|    ep_rew_mean          | 1.55      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 23        |\n",
      "|    iterations           | 29        |\n",
      "|    time_elapsed         | 2511      |\n",
      "|    total_timesteps      | 59392     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4340854 |\n",
      "|    clip_fraction        | 0.291     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.193    |\n",
      "|    explained_variance   | 0.56      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 42        |\n",
      "|    n_updates            | 280       |\n",
      "|    policy_gradient_loss | 0.205     |\n",
      "|    value_loss           | 91.6      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.41       |\n",
      "|    ep_rew_mean          | 84.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 2611       |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.33930105 |\n",
      "|    clip_fraction        | 0.387      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.454     |\n",
      "|    explained_variance   | 0.48       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 77.4       |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | 0.0114     |\n",
      "|    value_loss           | 496        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 20.9      |\n",
      "|    ep_rew_mean          | -5.87     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 23        |\n",
      "|    iterations           | 31        |\n",
      "|    time_elapsed         | 2697      |\n",
      "|    total_timesteps      | 63488     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9967922 |\n",
      "|    clip_fraction        | 0.593     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.304    |\n",
      "|    explained_variance   | -2.13     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 93.5      |\n",
      "|    n_updates            | 300       |\n",
      "|    policy_gradient_loss | 0.0198    |\n",
      "|    value_loss           | 212       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 16.4       |\n",
      "|    ep_rew_mean          | 18.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 2787       |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04211275 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.102     |\n",
      "|    explained_variance   | 0.469      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 278        |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | 0.0176     |\n",
      "|    value_loss           | 617        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.3        |\n",
      "|    ep_rew_mean          | 35.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 2880        |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018822808 |\n",
      "|    clip_fraction        | 0.0707      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.115      |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 275         |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | 0.00715     |\n",
      "|    value_loss           | 891         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 8.04       |\n",
      "|    ep_rew_mean          | 65.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 2975       |\n",
      "|    total_timesteps      | 69632      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06000221 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.18      |\n",
      "|    explained_variance   | 0.599      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 741        |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | 0.0236     |\n",
      "|    value_loss           | 1.7e+03    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.47       |\n",
      "|    ep_rew_mean          | 72.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 3071       |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13039201 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.144     |\n",
      "|    explained_variance   | 0.797      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 798        |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | 0.0125     |\n",
      "|    value_loss           | 1.47e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.37        |\n",
      "|    ep_rew_mean          | 84.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 3171        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021580894 |\n",
      "|    clip_fraction        | 0.0557      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.118      |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 197         |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0067     |\n",
      "|    value_loss           | 1.22e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.86        |\n",
      "|    ep_rew_mean          | 66.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 3268        |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011347979 |\n",
      "|    clip_fraction        | 0.0366      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.103      |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 802         |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | 0.00228     |\n",
      "|    value_loss           | 927         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.22        |\n",
      "|    ep_rew_mean          | 85.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 3370        |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012204645 |\n",
      "|    clip_fraction        | 0.0526      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.121      |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 349         |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00384    |\n",
      "|    value_loss           | 802         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.65        |\n",
      "|    ep_rew_mean          | 83.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 3467        |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015373405 |\n",
      "|    clip_fraction        | 0.0613      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.139      |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 432         |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.000613   |\n",
      "|    value_loss           | 625         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.29        |\n",
      "|    ep_rew_mean          | 85.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 3568        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019758077 |\n",
      "|    clip_fraction        | 0.0578      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.134      |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 353         |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00621    |\n",
      "|    value_loss           | 765         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 22.3      |\n",
      "|    ep_rew_mean          | -13.4     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 22        |\n",
      "|    iterations           | 41        |\n",
      "|    time_elapsed         | 3651      |\n",
      "|    total_timesteps      | 83968     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2359053 |\n",
      "|    clip_fraction        | 0.426     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.196    |\n",
      "|    explained_variance   | -0.388    |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 26.3      |\n",
      "|    n_updates            | 400       |\n",
      "|    policy_gradient_loss | 0.0741    |\n",
      "|    value_loss           | 86        |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.86       |\n",
      "|    ep_rew_mean          | 87.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 42         |\n",
      "|    time_elapsed         | 3739       |\n",
      "|    total_timesteps      | 86016      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.25395435 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.155     |\n",
      "|    explained_variance   | 0.328      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 134        |\n",
      "|    n_updates            | 410        |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    value_loss           | 415        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.5        |\n",
      "|    ep_rew_mean          | 84.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 3826       |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06993791 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.281     |\n",
      "|    explained_variance   | -0.444     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 71.9       |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | 0.059      |\n",
      "|    value_loss           | 326        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.48        |\n",
      "|    ep_rew_mean          | 84.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 3919        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027717624 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 38.2        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | 0.0298      |\n",
      "|    value_loss           | 165         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.43        |\n",
      "|    ep_rew_mean          | 84.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 4018        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032202125 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.209      |\n",
      "|    explained_variance   | 0.608       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 25          |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | 0.00335     |\n",
      "|    value_loss           | 67.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.2        |\n",
      "|    ep_rew_mean          | 86.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 22         |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 4113       |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02777278 |\n",
      "|    clip_fraction        | 0.0882     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.178     |\n",
      "|    explained_variance   | 0.786      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 11.8       |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | 0.00849    |\n",
      "|    value_loss           | 30         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.19        |\n",
      "|    ep_rew_mean          | 86          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 4207        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035578564 |\n",
      "|    clip_fraction        | 0.0955      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.13       |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.89        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00277    |\n",
      "|    value_loss           | 26.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.04        |\n",
      "|    ep_rew_mean          | 86.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 4303        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036763158 |\n",
      "|    clip_fraction        | 0.058       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.13       |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 10          |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | 0.0129      |\n",
      "|    value_loss           | 17          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.48       |\n",
      "|    ep_rew_mean          | 84.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 22         |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 4400       |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02919903 |\n",
      "|    clip_fraction        | 0.0596     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.115     |\n",
      "|    explained_variance   | 0.636      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 8.39       |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.00131   |\n",
      "|    value_loss           | 51.3       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x16ec2f0f070>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806191b5",
   "metadata": {},
   "source": [
    "This will start the training process of the PPO model for 100000 timesteps with the TrainAndLoggingCallback callback function that we have defined earlier. During the training process, the callback function will be called every 10000 timesteps to save the best model at that point in the CHECKPOINT_DIR directory, and also to log the training progress to the LOG_DIR directory using TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1261c2ef",
   "metadata": {},
   "source": [
    "## Step 6- Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f0f181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import eval policy to test agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e0a53f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model from disc\n",
    "model = PPO.load('./train/train_basic/best_model_60000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15215e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rendered environment\n",
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba9888e",
   "metadata": {},
   "source": [
    "we first created a non-rendered environment to train our agent, since rendering the screen is not necessary during the training process. Later, we created a rendered environment to evaluate the performance of our trained agent in a visually appealing way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "84f8566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Evaluate mean reward for 10 games\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aa695c",
   "metadata": {},
   "source": [
    "This code is evaluating the performance of the trained model on the VizDoom environment. Specifically, it is using the evaluate_policy function from Stable Baselines 3 library to calculate the mean reward obtained by the model over 100 episodes of the game. The model argument is the trained PPO model that we loaded from disk, the env argument is the VizDoom environment instance that we created with rendering enabled, and the n_eval_episodes argument specifies the number of evaluation episodes to run.\n",
    "\n",
    "The evaluate_policy function returns the mean reward obtained by the model and a dictionary containing additional evaluation metrics, but in this code, we only capture the mean reward value in the mean_reward variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2833850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.78"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3172ac",
   "metadata": {},
   "source": [
    "model.predict(obs) is used to predict the action to take given the current observation of the environment. It takes the observation of the environment as input and returns the predicted action as output. The action returned is based on the policy learned by the agent during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c517ce19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 87.0 is 0\n",
      "Total Reward for episode 95.0 is 1\n",
      "Total Reward for episode 68.0 is 2\n",
      "Total Reward for episode 95.0 is 3\n",
      "Total Reward for episode 19.0 is 4\n",
      "Total Reward for episode 71.0 is 5\n",
      "Total Reward for episode 95.0 is 6\n",
      "Total Reward for episode 52.0 is 7\n",
      "Total Reward for episode 95.0 is 8\n",
      "Total Reward for episode 95.0 is 9\n",
      "Total Reward for episode 95.0 is 10\n",
      "Total Reward for episode 95.0 is 11\n",
      "Total Reward for episode 95.0 is 12\n",
      "Total Reward for episode 95.0 is 13\n",
      "Total Reward for episode 95.0 is 14\n",
      "Total Reward for episode 95.0 is 15\n",
      "Total Reward for episode 95.0 is 16\n",
      "Total Reward for episode 95.0 is 17\n",
      "Total Reward for episode 95.0 is 18\n",
      "Total Reward for episode 95.0 is 19\n",
      "Total Reward for episode 75.0 is 20\n",
      "Total Reward for episode 95.0 is 21\n",
      "Total Reward for episode 75.0 is 22\n",
      "Total Reward for episode 95.0 is 23\n",
      "Total Reward for episode 67.0 is 24\n",
      "Total Reward for episode 79.0 is 25\n",
      "Total Reward for episode 95.0 is 26\n",
      "Total Reward for episode 95.0 is 27\n",
      "Total Reward for episode 59.0 is 28\n",
      "Total Reward for episode 79.0 is 29\n",
      "Total Reward for episode 95.0 is 30\n",
      "Total Reward for episode 95.0 is 31\n",
      "Total Reward for episode 95.0 is 32\n",
      "Total Reward for episode 83.0 is 33\n",
      "Total Reward for episode 71.0 is 34\n",
      "Total Reward for episode 95.0 is 35\n",
      "Total Reward for episode 60.0 is 36\n",
      "Total Reward for episode 95.0 is 37\n",
      "Total Reward for episode 95.0 is 38\n",
      "Total Reward for episode 95.0 is 39\n",
      "Total Reward for episode 95.0 is 40\n",
      "Total Reward for episode 95.0 is 41\n",
      "Total Reward for episode 75.0 is 42\n",
      "Total Reward for episode 71.0 is 43\n",
      "Total Reward for episode 95.0 is 44\n",
      "Total Reward for episode 95.0 is 45\n",
      "Total Reward for episode 58.0 is 46\n",
      "Total Reward for episode 56.0 is 47\n",
      "Total Reward for episode 79.0 is 48\n",
      "Total Reward for episode 95.0 is 49\n",
      "Total Reward for episode 95.0 is 50\n",
      "Total Reward for episode 75.0 is 51\n",
      "Total Reward for episode 95.0 is 52\n",
      "Total Reward for episode 95.0 is 53\n",
      "Total Reward for episode 67.0 is 54\n",
      "Total Reward for episode 76.0 is 55\n",
      "Total Reward for episode 56.0 is 56\n",
      "Total Reward for episode 95.0 is 57\n",
      "Total Reward for episode 95.0 is 58\n",
      "Total Reward for episode 95.0 is 59\n",
      "Total Reward for episode 95.0 is 60\n",
      "Total Reward for episode 95.0 is 61\n",
      "Total Reward for episode 83.0 is 62\n",
      "Total Reward for episode 67.0 is 63\n",
      "Total Reward for episode 95.0 is 64\n",
      "Total Reward for episode 83.0 is 65\n",
      "Total Reward for episode 68.0 is 66\n",
      "Total Reward for episode 95.0 is 67\n",
      "Total Reward for episode 95.0 is 68\n",
      "Total Reward for episode 95.0 is 69\n",
      "Total Reward for episode 52.0 is 70\n",
      "Total Reward for episode 95.0 is 71\n",
      "Total Reward for episode 75.0 is 72\n",
      "Total Reward for episode 87.0 is 73\n",
      "Total Reward for episode 95.0 is 74\n",
      "Total Reward for episode 95.0 is 75\n",
      "Total Reward for episode 95.0 is 76\n",
      "Total Reward for episode 95.0 is 77\n",
      "Total Reward for episode 95.0 is 78\n",
      "Total Reward for episode 56.0 is 79\n",
      "Total Reward for episode 95.0 is 80\n",
      "Total Reward for episode 95.0 is 81\n",
      "Total Reward for episode 95.0 is 82\n",
      "Total Reward for episode 95.0 is 83\n",
      "Total Reward for episode 83.0 is 84\n",
      "Total Reward for episode 95.0 is 85\n",
      "Total Reward for episode 95.0 is 86\n",
      "Total Reward for episode 75.0 is 87\n",
      "Total Reward for episode 79.0 is 88\n",
      "Total Reward for episode 95.0 is 89\n",
      "Total Reward for episode 60.0 is 90\n",
      "Total Reward for episode 95.0 is 91\n",
      "Total Reward for episode 95.0 is 92\n",
      "Total Reward for episode 72.0 is 93\n",
      "Total Reward for episode 95.0 is 94\n",
      "Total Reward for episode 95.0 is 95\n",
      "Total Reward for episode 41.0 is 96\n",
      "Total Reward for episode 52.0 is 97\n",
      "Total Reward for episode 75.0 is 98\n",
      "Total Reward for episode 95.0 is 99\n"
     ]
    }
   ],
   "source": [
    "for episode in range(100): \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # time.sleep(0.20)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(total_reward, episode))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca25b29",
   "metadata": {},
   "source": [
    "This code snippet is running 100 episodes of the game with the trained PPO model, where in each episode, the agent takes actions in the environment until it reaches a terminal state (done=True), and the total reward accumulated during that episode is printed. The time.sleep(2) function call between each episode adds a pause of 2 seconds between each episode for easier visualization of the gameplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96cd50fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(1, dtype=int64), None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c6519",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
